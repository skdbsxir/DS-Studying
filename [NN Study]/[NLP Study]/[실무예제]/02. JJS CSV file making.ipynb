{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "322797de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T08:29:00.609070Z",
     "start_time": "2021-09-06T08:29:00.595107Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a4081a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T08:29:01.794779Z",
     "start_time": "2021-09-06T08:29:01.775888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['네이버쇼핑_(단호박_다이어트식품)_cleaned.csv',\n",
       "  '네이버쇼핑_(단호박_샐러드)_cleaned.csv',\n",
       "  '네이버쇼핑_(단호박칩_반려동물)_cleaned.csv',\n",
       "  '스마트스토어_(구운_단호박)_cleaned.csv',\n",
       "  '스마트스토어_(다이어트_스낵)_cleaned.csv',\n",
       "  '스마트스토어_(호박_스낵)_cleaned.csv'],\n",
       " ['네이버쇼핑_(단호박_다이어트식품)_cleaned_keyword_neg.csv',\n",
       "  '네이버쇼핑_(단호박_다이어트식품)_cleaned_keyword_pos.csv',\n",
       "  '네이버쇼핑_(단호박_다이어트식품)_cleaned_keyword_total.csv',\n",
       "  '네이버쇼핑_(단호박_샐러드)_cleaned_keyword_neg.csv',\n",
       "  '네이버쇼핑_(단호박_샐러드)_cleaned_keyword_pos.csv',\n",
       "  '네이버쇼핑_(단호박_샐러드)_cleaned_keyword_total.csv',\n",
       "  '네이버쇼핑_(단호박칩_반려동물)_cleaned_keyword_neg.csv',\n",
       "  '네이버쇼핑_(단호박칩_반려동물)_cleaned_keyword_pos.csv',\n",
       "  '네이버쇼핑_(단호박칩_반려동물)_cleaned_keyword_total.csv',\n",
       "  '스마트스토어_(구운_단호박)_cleaned_keyword_neg.csv',\n",
       "  '스마트스토어_(구운_단호박)_cleaned_keyword_pos.csv',\n",
       "  '스마트스토어_(구운_단호박)_cleaned_keyword_total.csv',\n",
       "  '스마트스토어_(다이어트_스낵)_cleaned_keyword_neg.csv',\n",
       "  '스마트스토어_(다이어트_스낵)_cleaned_keyword_pos.csv',\n",
       "  '스마트스토어_(다이어트_스낵)_cleaned_keyword_total.csv',\n",
       "  '스마트스토어_(호박_스낵)_cleaned_keyword_neg.csv',\n",
       "  '스마트스토어_(호박_스낵)_cleaned_keyword_pos.csv',\n",
       "  '스마트스토어_(호박_스낵)_cleaned_keyword_total.csv'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'UseData/Voucher/Cleaned/'\n",
    "csvUrl = 'UseData/Voucher/WordExtract/'\n",
    "os.listdir(url), os.listdir(csvUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c8d430b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T08:29:03.922480Z",
     "start_time": "2021-09-06T08:29:03.888352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>네이버쇼핑_(단호박_다이어트식품)_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>네이버쇼핑_(단호박_샐러드)_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>네이버쇼핑_(단호박칩_반려동물)_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>스마트스토어_(구운_단호박)_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>스마트스토어_(다이어트_스낵)_cleaned.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>스마트스토어_(호박_스낵)_cleaned.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             File\n",
       "0  네이버쇼핑_(단호박_다이어트식품)_cleaned.csv\n",
       "1     네이버쇼핑_(단호박_샐러드)_cleaned.csv\n",
       "2   네이버쇼핑_(단호박칩_반려동물)_cleaned.csv\n",
       "3     스마트스토어_(구운_단호박)_cleaned.csv\n",
       "4    스마트스토어_(다이어트_스낵)_cleaned.csv\n",
       "5      스마트스토어_(호박_스낵)_cleaned.csv"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어짜피 내용이 똑같아서 굳이 나눌 필요가 없을거같은데\n",
    "filesDF = pd.DataFrame({'File' : os.listdir(url)})\n",
    "filesDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db81fe19",
   "metadata": {},
   "source": [
    "    긍/부정 별 키워드 csv 파일 생성 함수\n",
    "        -> JJS Tokenizing 04 참고."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ad9a076",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T08:36:42.811565Z",
     "start_time": "2021-09-06T08:36:42.784064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 115, 144)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사전 로딩\n",
    "stop_neg = 'UseData/Voucher/stoplist_neg.txt'\n",
    "stop_pos = 'UseData/Voucher/stoplist_pos.txt'\n",
    "\n",
    "# 긍정사전 로딩\n",
    "with open (stop_pos, 'r', encoding='utf8') as f:\n",
    "    stopwords_pos = []\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        stopwords_pos.append(line)\n",
    "        \n",
    "# 부정사전 로딩\n",
    "with open(stop_neg, 'r', encoding='utf8') as f2:\n",
    "    stopwords_neg = []\n",
    "    lines2 = f2.readlines()\n",
    "    for line in lines2:\n",
    "        line = line.strip()\n",
    "        stopwords_neg.append(line)\n",
    "\n",
    "# 이 사전을 같이 합친다? set 연산.\n",
    "temp = list((set(stopwords_pos)).difference(set(stopwords_neg))) # 차집합을 리스트로 구하고\n",
    "stopwords = stopwords_neg + temp # 빈 부분(교집합 부분)을 더하기.\n",
    "len(stopwords), len(stopwords_pos), len(stopwords_neg) # 약 중복되는 40개가 없어진듯."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "56174517",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T08:37:10.012707Z",
     "start_time": "2021-09-06T08:37:09.984081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('단호박', '단호박', '바중')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_pos[-3], stopwords_neg[-3], stopwords[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8ae53dd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T08:37:44.177506Z",
     "start_time": "2021-09-06T08:37:44.167536Z"
    }
   },
   "outputs": [],
   "source": [
    "# 앞서 정의한 불용어 사전 이용, 불용어 제거.\n",
    "def StopWordsEliminator(dataframe):\n",
    "    for words in dataframe['word']:\n",
    "        if words in stopwords:\n",
    "            dataframe.drop(dataframe[dataframe['word'] == words].index, inplace=True) # 불용어 row 제거\n",
    "            dataframe = dataframe.reset_index(drop=True) # 인덱스 재배치.\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2f3d521e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T08:37:45.348227Z",
     "start_time": "2021-09-06T08:37:45.319290Z"
    }
   },
   "outputs": [],
   "source": [
    "# 단어 추출\n",
    "def WordSelector(series):\n",
    "    wordList = []\n",
    "    for index in range(len(series)):\n",
    "        for word, pos in series[index]:\n",
    "            if (pos == 'Noun') or (pos == 'Adjective'):\n",
    "                wordList.append(word)\n",
    "    return wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4d93c8ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T08:37:46.453536Z",
     "start_time": "2021-09-06T08:37:46.415368Z"
    }
   },
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "\n",
    "def CSVMaker(url, file):\n",
    "    df = pd.read_csv(url + file, index_col=0) # 파일 1개씩 읽어와서\n",
    "    df.drop('제목', axis=1, inplace=True) # '제목' 컬럼은 사용 X.\n",
    "    \n",
    "    # 형태소 분리 및 태깅작업 먼저 진행\n",
    "    df['Contents'] = df['Contents'].apply(lambda x: okt.pos(x, norm=True, stem=True))\n",
    "    \n",
    "    # 긍/부정 프레임 나누기\n",
    "    df_pos = df.loc[df['긍부정']==1].reset_index()\n",
    "    df_pos.drop('index', axis=1, inplace=True)\n",
    "    df_neg = df.loc[df['긍부정']==0].reset_index()\n",
    "    df_neg.drop('index', axis=1, inplace=True)\n",
    "    \n",
    "    # 긍/부정 별로 데이터 프레임 생성\n",
    "    # 긍정은 500개 정도? 부정은 전부. (부정이 갯수가 더 적으니까.)\n",
    "    wordPosList = Counter(WordSelector(df_pos['Contents'])).most_common(500)\n",
    "    wordPosDF = pd.DataFrame(wordPosList)\n",
    "    wordPosDF.columns = wordPosDF.columns.astype(str)\n",
    "    wordPosDF = wordPosDF.rename(columns={'0':'word', '1':'freq'})\n",
    "    # 길이가 1인 단어들은 전부 제거\n",
    "    wordPosDF['TF'] = wordPosDF['word'].apply(lambda x: len(x) > 1)\n",
    "    wordPosDF.drop(wordPosDF[wordPosDF['TF'] == False].index, inplace=True)\n",
    "    wordPosDF = wordPosDF.reset_index(drop=True)\n",
    "    wordPosDF.drop('TF', axis=1, inplace=True)\n",
    "    wordPosDF = StopWordsEliminator(wordPosDF) # 사전을 통한 불용어 제거\n",
    "    \n",
    "    wordNegList = Counter(WordSelector(df_neg['Contents'])).most_common()\n",
    "    wordNegDF = pd.DataFrame(wordNegList)\n",
    "    wordNegDF.columns = wordNegDF.columns.astype(str)\n",
    "    wordNegDF = wordNegDF.rename(columns={'0':'word', '1':'freq'})\n",
    "    # 길이가 1인 단어들은 전부 제거\n",
    "    wordNegDF['TF'] = wordNegDF['word'].apply(lambda x: len(x) > 1)\n",
    "    wordNegDF.drop(wordNegDF[wordNegDF['TF'] == False].index, inplace=True)\n",
    "    wordNegDF = wordNegDF.reset_index(drop=True)\n",
    "    wordNegDF.drop('TF', axis=1, inplace=True)\n",
    "    wordNegDF = StopWordsEliminator(wordNegDF)\n",
    "    \n",
    "    # CSV파일로 저장\n",
    "    wordPosDF.to_csv(csvUrl + file[:-4] + '_keyword_pos.csv', index=True, header=True, encoding='utf-8-sig')\n",
    "    wordNegDF.to_csv(csvUrl + file[:-4] + '_keyword_neg.csv', index=True, header=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c571ed6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T07:25:39.620339Z",
     "start_time": "2021-09-06T07:25:35.885470Z"
    }
   },
   "outputs": [],
   "source": [
    "file = filesDF['File'][0]\n",
    "CSVMaker(url, file) # 만들어지는건 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "316f5794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T08:38:21.681414Z",
     "start_time": "2021-09-06T08:37:48.631253Z"
    }
   },
   "outputs": [],
   "source": [
    "for index in range(len(filesDF)):\n",
    "    file = filesDF['File'][index]\n",
    "    CSVMaker(url, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3547055d",
   "metadata": {},
   "source": [
    "    긍/부정 나눈거 말고도 그냥 통합본을 하나 만들까\n",
    "    키워드 한 500개 정도만 뽑아서?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "08d7286c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T08:38:26.037738Z",
     "start_time": "2021-09-06T08:38:26.021875Z"
    }
   },
   "outputs": [],
   "source": [
    "def TotalCSVMaker(url, file):\n",
    "    df = pd.read_csv(url + file, index_col=0) # 파일 1개씩 읽어와서\n",
    "    df.drop('제목', axis=1, inplace=True) # '제목' 컬럼은 사용 X.\n",
    "    \n",
    "    # 형태소 분리 및 태깅작업 먼저 진행\n",
    "    df['Contents'] = df['Contents'].apply(lambda x: okt.pos(x, norm=True, stem=True))\n",
    "    wordList = Counter(WordSelector(df['Contents'])).most_common(500)\n",
    "    wordDF = pd.DataFrame(wordList)\n",
    "    wordDF.columns = wordDF.columns.astype(str)\n",
    "    wordDF = wordDF.rename(columns={'0':'word', '1':'freq'})\n",
    "    wordDF['TF'] = wordDF['word'].apply(lambda x: len(x) > 1)\n",
    "    wordDF.drop(wordDF[wordDF['TF'] == False].index, inplace=True)\n",
    "    wordDF = wordDF.reset_index(drop=True)\n",
    "    wordDF.drop('TF', axis=1, inplace=True)\n",
    "    wordDF = StopWordsEliminator(wordDF)\n",
    "    \n",
    "    wordDF.to_csv(csvUrl + file[:-4] + '_keyword_total.csv', index=True, header=True, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955ce9be",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-06T08:38:29.180Z"
    }
   },
   "outputs": [],
   "source": [
    "for index in range(len(filesDF)):\n",
    "    file = filesDF['File'][index]\n",
    "    TotalCSVMaker(url, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdda59c",
   "metadata": {},
   "source": [
    "    단어-빈도 CSV 만들었고 이제\n",
    "    \n",
    "    1. WordCloud 그려보고\n",
    "    2. Word2Vec 돌려서 -> '단호박'처럼 특정 키워드 넣고 연관된 단어가 뭐 나오는지 확인?\n",
    "    3. LDA 돌려보기\n",
    "        ----- 2,3번은 모두 토큰화 진행된 단어들로 해야함."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'[WorkBase]'",
   "language": "python",
   "name": "workbase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
