{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 Chapter 02. 에서 토큰을 추출해 빈도를 세고 분류하는 방법을 살펴봤음.\n",
    "- 이런 토큰들로 다른 일을 해보자.\n",
    "    - **주어진 단어가 특정 문서 or 말뭉치 전체에서 얼마나 중요한지 측정**해보자. 단어의 중요도 Check.\n",
    "    - 긍정성 점수나 분류명이 있다면 그런 **단어들이 등장한 횟수를 나머지 모든 문서와 비교해 점수를 매겨**보자.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 앞서 얻었었던 이진 비트 벡터 형태로 표현하는 것이 아닌, **의미가 있는 연속된 값**들로 바꿔보자.\n",
    "    - 단어들이 이제 이산 공간이 아닌 **연속 공간에서 표현**.\n",
    "    - 뭐가 좋나? **좀 더 다양한 수학 도구들을 이용해 단어 표현을 다룰 수 있음**.\n",
    "    - 최종 목표는 **단어의 중요도 or 단어의 정보 내용을 반영한 수치 표현을 찾는 것**.\n",
    "        - 이번 장에선 **단어의 중요도를 반영하는 수치 표현**을 자세히 볼 것.\n",
    "        - Chapter 04. 에서는 수치로 표현한 단어 정보를 가지고 단어의 의미를 수량화 할 것.\n",
    "\n",
    "<br>\n",
    "\n",
    "용어 하나 짚고 넘어가자.  <br>\n",
    "**TF-IDF 벡터** : 단어의 중요도를 좀 더 잘 표현하는 단어 점수 벡터.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 빈도(도수)에 기초한다는 점에서 **통계적 모형**에 해당."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 모음\n",
    "\n",
    "<br>\n",
    "\n",
    "- 앞서 텍스트들을 통해 간단한 벡터 공간 모형을 만들었음.\n",
    "    - one-hot 같은 방법을 통해 이진 단어 모음(bag of words)을 만들고,\n",
    "    - 이 벡터를 DataFrame에 담아 문서 검색을 위한 index를 만들었음.\n",
    "- 단어 출현 횟수가 어떻게 도움이 되는지 예제를 한번 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:54:15.074840Z",
     "start_time": "2021-08-12T04:54:15.056850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'faster', 'harry', 'got', 'to', 'the', 'store', ',', 'the', 'faster', 'harry', ',', 'the', 'faster', ',', 'would', 'get', 'home', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'the': 4,\n",
       "         'faster': 3,\n",
       "         'harry': 2,\n",
       "         'got': 1,\n",
       "         'to': 1,\n",
       "         'store': 1,\n",
       "         ',': 3,\n",
       "         'would': 1,\n",
       "         'get': 1,\n",
       "         'home': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from collections import Counter # Counter 같은걸 bag or multiset 자료구조 라 부르기도 함.\n",
    "\n",
    "sentence = \"\"\"The faster Harry got to the store, the faster Harry, the faster, would get home.\"\"\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "bag_of_words = Counter(tokens)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Dict형태라 순서가 좀 뒤죽박죽.\n",
    "    key 순서는 단순히 저장, 갱신, 조회에 최적화. 일관된 출력 고려X.\n",
    "    그래서 기존 문장에 있던 단어 순서정보는 사라짐.\n",
    "    \n",
    "    빈도수 정렬을 위한 most_common이라는 메서드가 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:56:32.595700Z",
     "start_time": "2021-08-12T04:56:32.586720Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4), ('faster', 3), (',', 3), ('harry', 2)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.most_common(4) # 가장 많이 출현한 4개 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **어떤 단어가 한 문서에 출현한 횟수**를 **용어 빈도(term frequency, TF)**라 함.\n",
    "- 응용에 따라선 *TF를 해당 문서에 있는 모든 단어의 수로 나눠 정규화 하기도 함.*\n",
    "\n",
    "<br>\n",
    "\n",
    "- 4개의 토큰 중 **정관사 the와 문장부호 , 는 문장에 대한 정보를 많이 담고있지 않음**. **불용어**.\n",
    "- 가장 자주 등장한 토큰은 faster와 harry가 될 것.\n",
    "- Counter 객체 bag_of_words에서 harry의 TF를 조회해, 전체 토큰 수로 정규화 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T05:31:27.379631Z",
     "start_time": "2021-08-12T05:31:27.360631Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1818"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_harry_appears = bag_of_words['harry']\n",
    "num_unique_words = len(bag_of_words) # 원 문장에 있는 고유한 단어의 수\n",
    "tf = times_harry_appears / num_unique_words\n",
    "round(tf, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **정규화 된 용어 빈도(TF)**가 뭘까.\n",
    "    - **단어의 출현 횟수를 문서의 길이로 \"길들인\" 것**. 왜 길들여야 할까?\n",
    "    - dog라는 단어가 A문서에선 3번 등장하고, B문서에선 100번 등장한다 해보자.\n",
    "    - ***dog라는 단어가 B에서 더 중요하다 볼 수 있을까?*** 무조건 그렇진 않을 것. A와 B 문서 길이가 다를 수 있음.\n",
    "    - **단어가 포함된 문서의 길이도 고려해 단어의 중요도를 추정**할 필요가 있다.\n",
    "    - 이게 바로 정규화 된 용어 빈도.\n",
    "- 이렇게 **정규화된 TF는 주어진 단어가 그 문서에서 상대적으로 얼마나 중요한지도 말해줌.**\n",
    "    - 앞선 예시에서 harry의 속도가 중요함을 유추해 볼 수 있음.\n",
    "    - 이진값보다 출현 횟수가 단어 의미를 파악하는데 더 도움이 된다는 점을 알 수 있음.\n",
    "- 긴 문서를 사용한 예시를 봐보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T05:36:41.729210Z",
     "start_time": "2021-08-12T05:36:35.192034Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skdbs\\Anaconda3\\lib\\site-packages\\pugnlp\\constants.py:136: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  [datetime.datetime, pd.datetime, pd.Timestamp])\n",
      "C:\\Users\\skdbs\\Anaconda3\\lib\\site-packages\\pugnlp\\constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
      "C:\\Users\\skdbs\\Anaconda3\\lib\\site-packages\\pugnlp\\tutil.py:100: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "C:\\Users\\skdbs\\Anaconda3\\lib\\site-packages\\pugnlp\\util.py:80: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "INFO:nlpia.constants:Starting logger in nlpia.constants...\n",
      "C:\\Users\\skdbs\\Anaconda3\\lib\\site-packages\\nlpia\\futil.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "C:\\Users\\skdbs\\Anaconda3\\lib\\site-packages\\nlpia\\loaders.py:78: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  np = pd.np\n",
      "INFO:nlpia.loaders:No BIGDATA index found in C:\\Users\\skdbs\\Anaconda3\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv so copy C:\\Users\\skdbs\\Anaconda3\\lib\\site-packages\\nlpia\\data\\bigdata_info.latest.csv to C:\\Users\\skdbs\\Anaconda3\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv if you want to \"freeze\" it.\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('C:\\\\Users\\\\skdbs\\\\Anaconda3\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\mavis-batey-greetings.csv',), **{'low_memory': False})`...\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('C:\\\\Users\\\\skdbs\\\\Anaconda3\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\sms-spam.csv',), **{'low_memory': False})`...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'a': 20,\n",
       "         'kite': 16,\n",
       "         'is': 7,\n",
       "         'traditionally': 1,\n",
       "         'tethered': 2,\n",
       "         'heavier-than-air': 1,\n",
       "         'craft': 2,\n",
       "         'with': 2,\n",
       "         'wing': 5,\n",
       "         'surfaces': 1,\n",
       "         'that': 2,\n",
       "         'react': 1,\n",
       "         'against': 1,\n",
       "         'the': 26,\n",
       "         'air': 2,\n",
       "         'to': 5,\n",
       "         'create': 1,\n",
       "         'lift': 4,\n",
       "         'and': 10,\n",
       "         'drag.': 1,\n",
       "         'consists': 2,\n",
       "         'of': 10,\n",
       "         'wings': 1,\n",
       "         ',': 15,\n",
       "         'tethers': 2,\n",
       "         'anchors.': 2,\n",
       "         'kites': 8,\n",
       "         'often': 2,\n",
       "         'have': 4,\n",
       "         'bridle': 2,\n",
       "         'guide': 1,\n",
       "         'face': 1,\n",
       "         'at': 3,\n",
       "         'correct': 1,\n",
       "         'angle': 1,\n",
       "         'so': 3,\n",
       "         'wind': 2,\n",
       "         'can': 3,\n",
       "         'it.': 1,\n",
       "         \"'s\": 2,\n",
       "         'also': 3,\n",
       "         'may': 4,\n",
       "         'be': 5,\n",
       "         'designed': 2,\n",
       "         'not': 1,\n",
       "         'needed': 1,\n",
       "         ';': 2,\n",
       "         'when': 2,\n",
       "         'kiting': 3,\n",
       "         'sailplane': 1,\n",
       "         'for': 2,\n",
       "         'launch': 1,\n",
       "         'tether': 1,\n",
       "         'meets': 1,\n",
       "         'single': 1,\n",
       "         'point.': 1,\n",
       "         'fixed': 1,\n",
       "         'or': 6,\n",
       "         'moving': 2,\n",
       "         'untraditionally': 1,\n",
       "         'in': 7,\n",
       "         'technical': 2,\n",
       "         'tether-set-coupled': 1,\n",
       "         'sets': 1,\n",
       "         'even': 2,\n",
       "         'though': 1,\n",
       "         'system': 1,\n",
       "         'still': 1,\n",
       "         'called': 2,\n",
       "         'kite.': 1,\n",
       "         'sustains': 1,\n",
       "         'flight': 1,\n",
       "         'generated': 1,\n",
       "         'flows': 1,\n",
       "         'around': 1,\n",
       "         'surface': 2,\n",
       "         'producing': 1,\n",
       "         'low': 1,\n",
       "         'pressure': 2,\n",
       "         'above': 1,\n",
       "         'high': 1,\n",
       "         'below': 1,\n",
       "         'wings.': 1,\n",
       "         'interaction': 1,\n",
       "         'generates': 1,\n",
       "         'horizontal': 1,\n",
       "         'drag': 2,\n",
       "         'along': 1,\n",
       "         'direction': 1,\n",
       "         'wind.': 1,\n",
       "         'resultant': 1,\n",
       "         'force': 2,\n",
       "         'vector': 1,\n",
       "         'from': 1,\n",
       "         'components': 1,\n",
       "         'opposed': 1,\n",
       "         'by': 2,\n",
       "         'tension': 1,\n",
       "         'one': 1,\n",
       "         'more': 1,\n",
       "         'lines': 1,\n",
       "         'which': 2,\n",
       "         'attached.': 1,\n",
       "         'anchor': 1,\n",
       "         'point': 1,\n",
       "         'line': 1,\n",
       "         'static': 1,\n",
       "         '(': 1,\n",
       "         'e.g.': 1,\n",
       "         'towing': 1,\n",
       "         'running': 1,\n",
       "         'person': 1,\n",
       "         'boat': 1,\n",
       "         'free-falling': 1,\n",
       "         'anchors': 1,\n",
       "         'as': 5,\n",
       "         'paragliders': 1,\n",
       "         'fugitive': 1,\n",
       "         'parakites': 1,\n",
       "         'vehicle': 1,\n",
       "         ')': 1,\n",
       "         '.': 2,\n",
       "         'same': 1,\n",
       "         'principles': 1,\n",
       "         'fluid': 1,\n",
       "         'flow': 1,\n",
       "         'apply': 1,\n",
       "         'liquids': 1,\n",
       "         'are': 3,\n",
       "         'used': 2,\n",
       "         'under': 1,\n",
       "         'water.': 1,\n",
       "         'hybrid': 1,\n",
       "         'comprising': 1,\n",
       "         'both': 1,\n",
       "         'lighter-than-air': 1,\n",
       "         'balloon': 1,\n",
       "         'well': 1,\n",
       "         'lifting': 1,\n",
       "         'kytoon.': 1,\n",
       "         'long': 1,\n",
       "         'varied': 1,\n",
       "         'history': 1,\n",
       "         'many': 1,\n",
       "         'different': 1,\n",
       "         'types': 1,\n",
       "         'flown': 3,\n",
       "         'individually': 1,\n",
       "         'festivals': 1,\n",
       "         'worldwide.': 1,\n",
       "         'recreation': 1,\n",
       "         'art': 1,\n",
       "         'other': 1,\n",
       "         'practical': 1,\n",
       "         'uses.': 1,\n",
       "         'sport': 1,\n",
       "         'aerial': 1,\n",
       "         'ballet': 1,\n",
       "         'sometimes': 1,\n",
       "         'part': 1,\n",
       "         'competition.': 1,\n",
       "         'power': 2,\n",
       "         'multi-line': 1,\n",
       "         'steerable': 1,\n",
       "         'generate': 1,\n",
       "         'large': 1,\n",
       "         'forces': 1,\n",
       "         'activities': 1,\n",
       "         'such': 1,\n",
       "         'surfing': 1,\n",
       "         'landboarding': 1,\n",
       "         'fishing': 1,\n",
       "         'buggying': 1,\n",
       "         'new': 1,\n",
       "         'trend': 1,\n",
       "         'snow': 1,\n",
       "         'kiting.': 1,\n",
       "         'man-lifting': 1,\n",
       "         'been': 1,\n",
       "         'made': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nlpia.data.loaders import kite_text\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(kite_text.lower())\n",
    "tokenCounts = Counter(tokens)\n",
    "tokenCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    상위 토큰에 불용어가 많이 보임. a, is 같은거.\n",
    "    불용어 제거하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T05:39:38.539791Z",
     "start_time": "2021-08-12T05:39:38.291071Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T05:41:03.251351Z",
     "start_time": "2021-08-12T05:41:03.236360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'kite': 16,\n",
       "          'traditionally': 1,\n",
       "          'tethered': 2,\n",
       "          'heavier-than-air': 1,\n",
       "          'craft': 2,\n",
       "          'wing': 5,\n",
       "          'surfaces': 1,\n",
       "          'react': 1,\n",
       "          'air': 2,\n",
       "          'create': 1,\n",
       "          'lift': 4,\n",
       "          'drag.': 1,\n",
       "          'consists': 2,\n",
       "          'wings': 1,\n",
       "          ',': 15,\n",
       "          'tethers': 2,\n",
       "          'anchors.': 2,\n",
       "          'kites': 8,\n",
       "          'often': 2,\n",
       "          'bridle': 2,\n",
       "          'guide': 1,\n",
       "          'face': 1,\n",
       "          'correct': 1,\n",
       "          'angle': 1,\n",
       "          'wind': 2,\n",
       "          'it.': 1,\n",
       "          \"'s\": 2,\n",
       "          'also': 3,\n",
       "          'may': 4,\n",
       "          'designed': 2,\n",
       "          'needed': 1,\n",
       "          ';': 2,\n",
       "          'kiting': 3,\n",
       "          'sailplane': 1,\n",
       "          'launch': 1,\n",
       "          'tether': 1,\n",
       "          'meets': 1,\n",
       "          'single': 1,\n",
       "          'point.': 1,\n",
       "          'fixed': 1,\n",
       "          'moving': 2,\n",
       "          'untraditionally': 1,\n",
       "          'technical': 2,\n",
       "          'tether-set-coupled': 1,\n",
       "          'sets': 1,\n",
       "          'even': 2,\n",
       "          'though': 1,\n",
       "          'system': 1,\n",
       "          'still': 1,\n",
       "          'called': 2,\n",
       "          'kite.': 1,\n",
       "          'sustains': 1,\n",
       "          'flight': 1,\n",
       "          'generated': 1,\n",
       "          'flows': 1,\n",
       "          'around': 1,\n",
       "          'surface': 2,\n",
       "          'producing': 1,\n",
       "          'low': 1,\n",
       "          'pressure': 2,\n",
       "          'high': 1,\n",
       "          'wings.': 1,\n",
       "          'interaction': 1,\n",
       "          'generates': 1,\n",
       "          'horizontal': 1,\n",
       "          'drag': 2,\n",
       "          'along': 1,\n",
       "          'direction': 1,\n",
       "          'wind.': 1,\n",
       "          'resultant': 1,\n",
       "          'force': 2,\n",
       "          'vector': 1,\n",
       "          'components': 1,\n",
       "          'opposed': 1,\n",
       "          'tension': 1,\n",
       "          'one': 1,\n",
       "          'lines': 1,\n",
       "          'attached.': 1,\n",
       "          'anchor': 1,\n",
       "          'point': 1,\n",
       "          'line': 1,\n",
       "          'static': 1,\n",
       "          '(': 1,\n",
       "          'e.g.': 1,\n",
       "          'towing': 1,\n",
       "          'running': 1,\n",
       "          'person': 1,\n",
       "          'boat': 1,\n",
       "          'free-falling': 1,\n",
       "          'anchors': 1,\n",
       "          'paragliders': 1,\n",
       "          'fugitive': 1,\n",
       "          'parakites': 1,\n",
       "          'vehicle': 1,\n",
       "          ')': 1,\n",
       "          '.': 2,\n",
       "          'principles': 1,\n",
       "          'fluid': 1,\n",
       "          'flow': 1,\n",
       "          'apply': 1,\n",
       "          'liquids': 1,\n",
       "          'used': 2,\n",
       "          'water.': 1,\n",
       "          'hybrid': 1,\n",
       "          'comprising': 1,\n",
       "          'lighter-than-air': 1,\n",
       "          'balloon': 1,\n",
       "          'well': 1,\n",
       "          'lifting': 1,\n",
       "          'kytoon.': 1,\n",
       "          'long': 1,\n",
       "          'varied': 1,\n",
       "          'history': 1,\n",
       "          'many': 1,\n",
       "          'different': 1,\n",
       "          'types': 1,\n",
       "          'flown': 3,\n",
       "          'individually': 1,\n",
       "          'festivals': 1,\n",
       "          'worldwide.': 1,\n",
       "          'recreation': 1,\n",
       "          'art': 1,\n",
       "          'practical': 1,\n",
       "          'uses.': 1,\n",
       "          'sport': 1,\n",
       "          'aerial': 1,\n",
       "          'ballet': 1,\n",
       "          'sometimes': 1,\n",
       "          'part': 1,\n",
       "          'competition.': 1,\n",
       "          'power': 2,\n",
       "          'multi-line': 1,\n",
       "          'steerable': 1,\n",
       "          'generate': 1,\n",
       "          'large': 1,\n",
       "          'forces': 1,\n",
       "          'activities': 1,\n",
       "          'surfing': 1,\n",
       "          'landboarding': 1,\n",
       "          'fishing': 1,\n",
       "          'buggying': 1,\n",
       "          'new': 1,\n",
       "          'trend': 1,\n",
       "          'snow': 1,\n",
       "          'kiting.': 1,\n",
       "          'man-lifting': 1,\n",
       "          'made': 1}),\n",
       " [('kite', 16), (',', 15), ('kites', 8), ('wing', 5), ('lift', 4)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "tokens = [x for x in tokens if x not in stopwords]\n",
    "kite_counts = Counter(tokens)\n",
    "kite_counts, kite_counts.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    kite, wing, lift 가 문서에서 중요한 단어임을 유추해 볼 수 있음.\n",
    "    다른 문서에서도 이와 비슷한 TF가 나오면, 해당 말뭉치의 모든 문서가\n",
    "    연이나 연날리기에 관한 문서일 가능성이 있다.\n",
    "    \n",
    "    그러면 아마 모든 문서가 string과 wind를 자주 언급할 것이며, \n",
    "    모든 문서에서 string의 TF와 wind의 TF가 높게 나올 것.\n",
    "    \n",
    "    이런 수치들을 수학 연산에 좀 더 적합한 형태로 바꿔보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "<br>\n",
    "\n",
    "- 수치들을 Dict에 담아두기만 했음. 활용할 건덕지가 많지 않다.\n",
    "- 문서를 표현하는 TF를 벡터로 만들어보자.\n",
    "- Dict구조에 담긴 TF들로 하나의 정규화된 TF벡터를 생성하는 예시를 한번 봐보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T05:49:58.506028Z",
     "start_time": "2021-08-12T05:49:58.488025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07207207207207207,\n",
       " 0.06756756756756757,\n",
       " 0.036036036036036036,\n",
       " 0.02252252252252252,\n",
       " 0.018018018018018018,\n",
       " 0.018018018018018018,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.009009009009009009,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045,\n",
       " 0.0045045045045045045]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector = []\n",
    "doc_length = len(tokens)\n",
    "for key, value in kite_counts.most_common():\n",
    "    document_vector.append(value / doc_length)\n",
    "document_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    수학 연산을 위한 새 문서를 추가하고, 어휘집을 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T05:52:39.716096Z",
     "start_time": "2021-08-12T05:52:39.704102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The faster Harry got to the store, the faster Harry, the faster, would get home.',\n",
       " 'Harry is hairy and faster than Jill.',\n",
       " 'Jill is not as hairy as Harry.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\"\"\"The faster Harry got to the store, the faster Harry, the faster, would get home.\"\"\"]\n",
    "docs.append(\"\"\"Harry is hairy and faster than Jill.\"\"\")\n",
    "docs.append(\"\"\"Jill is not as hairy as Harry.\"\"\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T05:57:49.062228Z",
     "start_time": "2021-08-12T05:57:49.044224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',',\n",
       " '.',\n",
       " 'and',\n",
       " 'as',\n",
       " 'faster',\n",
       " 'get',\n",
       " 'got',\n",
       " 'hairy',\n",
       " 'harry',\n",
       " 'home',\n",
       " 'is',\n",
       " 'jill',\n",
       " 'not',\n",
       " 'store',\n",
       " 'than',\n",
       " 'the',\n",
       " 'to',\n",
       " 'would']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_tokens = []\n",
    "for doc in docs:\n",
    "    doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]\n",
    "\n",
    "\"\"\"\n",
    "doc_tokens\n",
    "\n",
    "[[',',\n",
    "  ',',\n",
    "  ',',\n",
    "  '.',\n",
    "  'faster',\n",
    "  'faster',\n",
    "  'faster',\n",
    "  'get',\n",
    "  'got',\n",
    "  'harry',\n",
    "  'harry',\n",
    "  'home',\n",
    "  'store',\n",
    "  'the',\n",
    "  'the',\n",
    "  'the',\n",
    "  'the',\n",
    "  'to',\n",
    "  'would'],\n",
    " ['.', 'and', 'faster', 'hairy', 'harry', 'is', 'jill', 'than'],\n",
    " ['.', 'as', 'as', 'hairy', 'harry', 'is', 'jill', 'not']]\n",
    "\"\"\"\n",
    "all_doc_tokens = sum(doc_tokens, []) # 토큰 전부 합치기\n",
    "lexicon = sorted(set(all_doc_tokens)) # 어휘사전 구축\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문서를 표현하는 벡터의 길이는 항상 어휘집의 단어 수와 같아야 함.\n",
    "- 벡터의 한 성분의 위치는 어휘집 안에서의 그 성분에 대응되는 토큰의 위치와 일치해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T05:58:52.354685Z",
     "start_time": "2021-08-12T05:58:52.335697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0),\n",
       "             ('.', 0),\n",
       "             ('and', 0),\n",
       "             ('as', 0),\n",
       "             ('faster', 0),\n",
       "             ('get', 0),\n",
       "             ('got', 0),\n",
       "             ('hairy', 0),\n",
       "             ('harry', 0),\n",
       "             ('home', 0),\n",
       "             ('is', 0),\n",
       "             ('jill', 0),\n",
       "             ('not', 0),\n",
       "             ('store', 0),\n",
       "             ('than', 0),\n",
       "             ('the', 0),\n",
       "             ('to', 0),\n",
       "             ('would', 0)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "zero_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 말뭉치의 문서마다 기준 벡터를 복사해, 각 성분을 해당 용어 빈도TF로 갱신해 세 문서의 벡터를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T06:03:25.135895Z",
     "start_time": "2021-08-12T06:03:25.120904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([(',', 0.16666666666666666),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.16666666666666666),\n",
       "              ('get', 0.05555555555555555),\n",
       "              ('got', 0.05555555555555555),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.1111111111111111),\n",
       "              ('home', 0.05555555555555555),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.05555555555555555),\n",
       "              ('than', 0),\n",
       "              ('the', 0.2222222222222222),\n",
       "              ('to', 0.05555555555555555),\n",
       "              ('would', 0.05555555555555555)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.05555555555555555),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.05555555555555555),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('home', 0),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.05555555555555555),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('home', 0),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('not', 0.05555555555555555),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "doc_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector) # 벡터를 복사해 독립적인 벡터 생성.\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value / len(lexicon)\n",
    "    doc_vectors.append(vec)\n",
    "doc_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    이 벡터들을 어떻게 활용할 수 있을까?\n",
    "    코사인 유사도를 통해 문서 간 유사도 측정을 할 수 있을 것.\n",
    "    다른말로, 벡터에 적용되는 여러 수학 연산을 사용할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Space\n",
    "\n",
    "<br>\n",
    "\n",
    "- **벡터**는 **순서 있는 수치 목록**. 수치들은 벡터 공간에서 벡터의 위치를 말해주는 **좌표성분**으로 볼 수 있음.\n",
    "    - **하나의 벡터는 공간의 한 장소(위치)를 나타냄**.\n",
    "    - or, 벡터로 방향이나 속력, 두 위치사이의 거리를 나타낼 수도 있음.\n",
    "- **벡터공간**은 그 **공간 안에 나타날 수 있는 모든 가능한 벡터의 집합**.\n",
    "    - **벡터의 성분 갯수는 해당 벡터 공간의 차원 수**.\n",
    "    - 성분이 2개인 벡터는 2차원 벡터 공간에 놓이고, 3개인 벡터는 3차원 벡터 공간에 놓인다.\n",
    "- 여기서 다룰 벡터공간은 *공간의 모든 축이 직선이고 서로 직교하는 유클리드 공간*에 존재.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 자연어 문서에서 **벡터 공간의 차원 수**는 **전체 말뭉치에 등장하는 고유한 단어들의 개수**.\n",
    "    - TF 벡터나 TF-IDF 벡터를 다룰 땐 이 **고유 단어 수를 대문자 $K$**로 표현.\n",
    "    - 고유 단어 수는 말뭉치의 어휘(vocabulary)크기이기도 하므로, $|V|$로 표현하기도 함.\n",
    "- 정리하면, **말뭉치의 모든 문서는 $K$차원 벡터 공간의 $K$차원 벡터들로 표현된다.**\n",
    "    - 앞서 봤던 Harry 문서의 경우는 $K=18$.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 문서를 하나의 공간을 기준으로 한 벡터로 표현하면 문서들의 유사도를 수치로 계산할 수 있음.\n",
    "- 두 벡터간의 거리는 한 벡터의 머리에서 다른 벡터의 머리까지의 거리. 이를 2-norm 거리라고 함.\n",
    "- 하지만 **단어 출현 횟수 벡터(TF 벡터)들은 이런식으로 거리를 측정하면 안됨.**\n",
    "\n",
    "<br>\n",
    "\n",
    "- 서로 가까운 (거리가 짧은) 두 벡터는 서로 비슷하다 고 할 수 있고, ***두 문서의 벡터 표현들이 비슷하면 두 문서는 서로 비슷하다고 볼 수 있음***.\n",
    "- 하지만 벡터 거리 정의에 따르면, **벡터가 가르키는 방향이 비슷할수록, 두 벡터의 길이가 비슷할수록 두 벡터의 거리가 줄어듦.**\n",
    "    - *비슷한 길이의 두 문서에서 얻은 TF벡터들은 길이가 비슷할 것이며, 두 벡터간 거리가 짧게 나옴.*\n",
    "    - 뭐가 문제냐? **두 문서의 길이가 같다고 해서 두 문서가 비슷한건 아님.**\n",
    "    - 그럼? **비슷한 단어들을 비슷한 빈도로 사용했는지를 측정**하는 것이 더 바람직. \n",
    "    - 여기서 필요한게? **각도 $\\theta$**. ***이를 이용해 유사도를 측정***할 수 있음. 이것이 바로 **코사인 유사도**.\n",
    "    \n",
    "    \\begin{align}\n",
    "    A \\cdot B = |A| |B| * \\cos(\\theta) \\\\\n",
    "    \\Rightarrow \\cos(\\theta) = \\frac{A \\cdot B}{|A| |B|} \\\\\n",
    "    \\end{align}\n",
    "    \n",
    "- 유사도의 범위는 -1 ~ +1 까지."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    코사인 유사도를 계산하는 함수를 만들어서 써보자.\n",
    "    (나중에 패키지로 쓰지만, 원리는 알고 넘어가보자.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T06:40:47.681295Z",
     "start_time": "2021-08-12T06:40:47.662305Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    vec1 = [val for val in vec1.values()]\n",
    "    vec2 = [val for val in vec2.values()]\n",
    "    dot_prod = 0\n",
    "    \n",
    "    # 내적 계산\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "    # 노름 계산\n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "    \n",
    "    # 유사도 계산\n",
    "    return dot_prod / (mag_1 * mag_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두 벡터의 내적은 더 짧은 벡터를 긴 벡터에 수직으로 투영해 나온 벡터의 길이이기도 함.\n",
    "    - 벡터가 같은 방향을 가리킬수록 내적값은 큰 양수, 반대일수록 큰 음수값.\n",
    "    - **코사인 유사도는 이런 내적을 길이에 맞게 정규화 한 것.**\n",
    "        - **유사도가 1**이라는 의미는 **두 벡터가 모든 차원에서 완전히 같은방향**을 가르킨다는 의미.\n",
    "        - **유사도가 0**이라는 의미는 **두 벡터에 공통점이 없다**는 의미. **모든 차원에서 수직**.\n",
    "            - NLP의 경우 **두 문서에 공통으로 출현하는 단어가 하나도 없으면 해당 TF벡터가 0**이 됨.\n",
    "            - 그러면 **두 문서는 주제나 내용이 상당히 다를 가능성이 큼.**\n",
    "        - **유사도가 -1**이라는 의미는 **두 벡터가 완전히 다른 방향**. 두 벡터중 하나가 다른 사분면에 있는 경우.\n",
    "            - 근데 ***TF벡터들은 같은 사분면에 있으므로 이런 일(음수값이 나오는 현상)은 안생김.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
