{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문서에서 토큰을 추출하기 위해선 str.split()과 같거나 그 이상의 기능을 수행하는 조작 메서드가 필요.\n",
    "    - *we'll 과 같은 축약형을 we, will로 분리할 매커니즘이 필요.*\n",
    "- 토큰을 모두 식별했다면 **어간 추출(stemming)**이 필요. **정규 표현식을 이용**해서 가능함.\n",
    "- 다음으로 이 **토큰들을 이용해 단어 모음(bag of words)이라 부르는 문서의 벡터 표현을 생성.**\n",
    "- ***단어 또는 토큰이 무엇을 표현할까?*** 문맥과 상황에 따라 다를 것.\n",
    "    - don't를 하나의 의미 전달 단위로 볼 수도 있고, 2개의 단위로 볼 수도 있음.\n",
    "    - ice cream같은 경우는 'ice cream'을 하나로 봐야할까, 'ice' + 'cream'으로 봐야 할까?\n",
    "- 텍스트를 단어로 분할하기 위해선 어떤 매커니즘이 필요할까?\n",
    "    - 한 단어로 구성된 **'Don't!'**를 보자.\n",
    "    - 컴퓨터 측면에서 보면 이는 **단순히 1개의 단어**\n",
    "    - 하지만 사람 측면에서 보면 **어떤 생략된 단어가 있음**을 알 수 있음.\n",
    "        - 'Don't you dare!' 아니면 'Don't you do that!'을 줄인 것 일 것.\n",
    "        - 이런 축약형 단어는 추후 chapter.4에서 살펴보자.\n",
    "        - 지금은 우선 토큰들을 주어진대로 식별하는 토큰 생성기를 구축해보자.\n",
    "- **n-gram** : ***문자열에서 추출된 n쌍의 토큰(단어벡터).***\n",
    "    - 이를 이용하면 'ice cream'같은 단어를 'ice', 'cream', 'ice cream'으로 추출할 수 있음.\n",
    "        - **벡터 표현이 1개가 아닌 2개(더 나아가 n개)로 이뤄지는 것.**\n",
    "- 텍스트를 수치 벡터로 변환하는 것은 손실이 큰 추출과정.\n",
    "    - But, 텍스트에 담긴 정보를 통해 ML모형을 구축할 수 있음. 어느정도의 정보는 유지하는 셈."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming Overview  <br>\n",
    "\n",
    "- **어간 추출(stemming)**은 **한 단어의 여러 변형을 동일한 '통' or '군집'으로 묶는 것**을 의미.\n",
    "- ***복수형을 단수형으로 바꾸기 위해 s를 그냥 지워도 될까?***\n",
    "    - words에서 s를 지워 word를 얻을 수 있음.\n",
    "    - bus에서 s를 지우면 bu. 이 방법이 통하지 않음.\n",
    "    - 어떠한 매커니즘이 필요.\n",
    "- **한 단어 또는 단어의 한 부분에 있는 개별 글자가 그 단어의 의미에 관한 정보를 제공함.**\n",
    "- **글자 하나 때문에 단어의 의미가 완전히 달라질 수도 있음.**\n",
    "- 전통적인 어간 추출 방식을 사용해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary building with Token Generator\n",
    "\n",
    "<br>\n",
    "\n",
    "- **토큰화(Tokenization)** : 문서 분할(segmentation)의 한 종류. **주어진 텍스트를 더 작고 구체적인 정보를 담은 조각으로 쪼개는 것.**\n",
    "- 텍스트를 토큰으로 나누는 분할을 Tokenization 또는 Token Generation이라 함.\n",
    "- NLP의 기본 구축요소들에 **컴파일러 구성 요소를 다음과 같이 대응**할 수 있음.\n",
    "    - Token Generator == Scanner, Lexer\n",
    "    - Vocabulary == Vocabulary Dictionary (어휘집)\n",
    "    - Parser == Compiler\n",
    "    - Token, n-gram, etc... == Token, 기호\n",
    "- **토큰화는 NLP 파이프라인 첫 단계**인 만큼 파이프라인 나머지 부분에 큰 영향을 미침.\n",
    "    - **구조가 없는 자료인 자연어 텍스트를 정보 조각들로 분할.**\n",
    "    - 이런 **정보 조각들을 개별 요소로 취급해 개수를 셀 수 있음.**\n",
    "    - 이 **횟수를 그대로 벡터 성분으로 이용**해 벡터를 만들면 **문서를 대표하는 하나의 벡터 표현**이 됨.\n",
    "        - 이렇게 생성된 **단어모음 벡터의 가장 일반적인 용도는 문서 검색.**\n",
    "\n",
    "<br>\n",
    "\n",
    "- 토큰화의 가장 간단한 방법은 **문자열 안 공백(whitespace)을 구분자(delimiter)로 사용**하는 것. **str.split()**\n",
    "    - **한국어** 같은 경우는 **단어를 좀 더 세부적인 단위인 형태소로 분리하는 것이 나을 수도** 있음.\n",
    "    - **명사들만 추출하고 조사들은 불용어로 처리**하는 것이 바람직한 경우들. (-이, -을 제거)\n",
    "\n",
    "\n",
    "               split을 이용한 간단한 토큰 분할을 보자.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T00:30:23.570741Z",
     "start_time": "2021-08-11T00:30:23.560732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Charlie',\n",
       " 'started',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'study',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '23.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Charlie started Data Science study at the age of 23.'\n",
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문장 끝 23. 에 마침표가 포함되어 있는것만 빼면 적당히 나눠졌음.\n",
    "    - **마침표 같은 문장 부호는 의미 있는 토큰과 분리하는 것이 원칙.**\n",
    "        - 26. 이라는 토큰은 컴퓨터 언어에서 부동소수점 수 26.0을 나타내는덴 완벽.\n",
    "        - But, 다른 단어 26들과는 구분되는 다른 토큰이 되어버림.\n",
    "        - 26. 26! 26? 들의 문장 부호를 제거해 모두 26이 되도록 하는것이 좋을 것.\n",
    "    - 좀 더 정교하게 한다면 이런 **문장부호들을 별개의 토큰으로 분리**할 수 있을 것.\n",
    "- 문장부호 제거는 추후 진행하고, **각 단어를 나타내는 수치 벡터 표현**을 만들어보자.\n",
    "    - 이를 **one-hot vector**라고 함.\n",
    "    - **원핫 벡터들의 시퀀스는 기존의 원문 텍스트를 온전히 표현**할 수 있음.\n",
    "    - 이 과정을 통해 **주어진 텍스트가 수치적인 자료구조(2차원 수치배열)로 변환**됨.\n",
    "        - NLP의 첫번째 문제인 단어들을 수치들로 변환하는 문제가 여기서 해결되는 셈."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T00:30:25.610743Z",
     "start_time": "2021-08-11T00:30:25.604748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23., Charlie, Data, Science, age, at, of, started, study, the'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "token_sequence = sentence.split()\n",
    "\n",
    "# 고유 단어들만을 모아 어휘사전을 만듦.\n",
    "vocab = sorted(set(token_sequence))\n",
    "', '.join(vocab) # 토큰을 숫자>영대문자>영소문자 순으로 정렬."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T00:30:27.426520Z",
     "start_time": "2021-08-11T00:30:27.413733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23. Charlie Data Science age at of started study the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens = len(token_sequence)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 너비가 어휘의 고유 단어 개수, 높이가 문서의 길이(토큰개수)인 빈 테이블(2차원 배열) 생성.\n",
    "# 현재 예시에선 10*10 크기의 테이블이 생성됨.\n",
    "onehot_vectors = np.zeros((num_tokens, vocab_size), int)\n",
    "\n",
    "# 문장의 각 단어에 대해, 어휘에서 그 단어에 해당하는 위치의 테이블 값을 1로 설정.\n",
    "for i, word in enumerate(token_sequence):\n",
    "    onehot_vectors[i, vocab.index(word)] = 1\n",
    "\n",
    "print(' '.join(vocab))\n",
    "onehot_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 0과 1들이 문장의 원핫 단어 벡터들. 근데 뭐가 뭔지 알아보기가 힘듦.\n",
    "- 그러니 pd.DataFrame으로 그려서 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T00:30:29.758399Z",
     "start_time": "2021-08-11T00:30:29.096044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>23.</th>\n",
       "      <th>Charlie</th>\n",
       "      <th>Data</th>\n",
       "      <th>Science</th>\n",
       "      <th>age</th>\n",
       "      <th>at</th>\n",
       "      <th>of</th>\n",
       "      <th>started</th>\n",
       "      <th>study</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   23.  Charlie  Data  Science  age  at  of  started  study  the\n",
       "0    0        1     0        0    0   0   0        0      0    0\n",
       "1    0        0     0        0    0   0   0        1      0    0\n",
       "2    0        0     1        0    0   0   0        0      0    0\n",
       "3    0        0     0        1    0   0   0        0      0    0\n",
       "4    0        0     0        0    0   0   0        0      1    0\n",
       "5    0        0     0        0    0   1   0        0      0    0\n",
       "6    0        0     0        0    0   0   0        0      0    1\n",
       "7    0        0     0        0    1   0   0        0      0    0\n",
       "8    0        0     0        0    0   0   1        0      0    0\n",
       "9    1        0     0        0    0   0   0        0      0    0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(onehot_vectors, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원핫 벡터는 모든 성분 중 단 하나만 0이 아닌 대단히 sparse한 벡터.\n",
    "    - 문장이 커지면 메모리 낭비가 매~우 심해짐.\n",
    "- 좀더 보기 편하려면 **0 $\\rightarrow$ ' '(공백)**으로 바꿔서 봐보자. 파이프라인 넣을때 이런 짓 하면 안됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T00:30:31.401796Z",
     "start_time": "2021-08-11T00:30:31.335518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>23.</th>\n",
       "      <th>Charlie</th>\n",
       "      <th>Data</th>\n",
       "      <th>Science</th>\n",
       "      <th>age</th>\n",
       "      <th>at</th>\n",
       "      <th>of</th>\n",
       "      <th>started</th>\n",
       "      <th>study</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  23. Charlie Data Science age at of started study the\n",
       "0           1                                         \n",
       "1                                          1          \n",
       "2                1                                    \n",
       "3                        1                            \n",
       "4                                                1    \n",
       "5                               1                     \n",
       "6                                                    1\n",
       "7                            1                        \n",
       "8                                  1                  \n",
       "9   1                                                 "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(onehot_vectors, columns=vocab)\n",
    "df[df == 0] = ' '\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문장 1개만 있는 문서의 이런 표현에서, **각 row는 한 단어에 대한 벡터.**\n",
    "- 현재 문장은 10개의 단어, 각 단어는 한번씩만 사용 됐음.\n",
    "    - **한 col의 1은 해당 어휘 단어가 문서의 해당 위치에 출현한다는 의미.**\n",
    "    - **문서의 세번째 단어가 뭔지 알고싶으면 테이블의 3번째 row에서 값이 1인 col**을 찾으면 됨.\n",
    "        - 3번째 단어를 알고싶으니 **row index==2 에서 value가 1인 col을 찾으면 Data.**\n",
    "        - 실제 **sentence를 보면 Data가 3번째에 출현**함.\n",
    "        - NLP 파이프라인에서 문장의 한 단어 'Data'는 [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]라는 벡터가 되는 셈.\n",
    "- 단어의 벡터표현, 문서의 테이블 표현이 가진 **한 가지 장점은 어떤 정보도 소실되지 않는다는 것.**\n",
    "    - **각 col이 어떤 단어에 대응되는지에 대한 정보만 유지하면 원핫 벡터 테이블을 통해 元 문서 복원이 가능.**\n",
    "    - 이런 장점때문에 NN 같은데서 원핫 단어 벡터를 많이 사용함. 물론 메모리와 trade-off 해야하지만...\n",
    "- 원핫 벡터 테이블을 **피아노 롤** or **음악상자 원통** 같은 개념으로 생각할 수 있음.\n",
    "- 궁극적으로 하고자 하는것은 원핫 벡터들을 이용해 새로운 문장을 만드는것. LSTM을 이용한 NN 같은 모형을 사용해 만들어낼 수 있음. 추후 chapter 9, 10. 에서."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원핫 벡터 테이블은 컴퓨터가 좋아하는 **이진수**. 또한 **원문의 모든 세부 사항을 유지**하고 있음.\n",
    "- 하지만 **문장이 짧음에도 불구하고 테이블이 상당히 큼**. $\\rightarrow$ **차원축소** 과정이 필요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 우리가 원하는 것은 **문서의 의미를 압축해 그 본질을 추출**하는 것.\n",
    "    - **완벽한 복원 능력을 포기**하고 문서 하나를 커다란 테이블이 아닌 **벡터 하나로 압축**하자.\n",
    "- 이때 사용하는 것이 **단어 빈도(frequency) 벡터**. or **단어 모음 벡터**.\n",
    "    - 원핫벡터와는 다르게 **단어 순서에 관한 정보가 없음**. 단어의 위치가 아닌 **출현 횟수**만을 담고 있음.\n",
    "    - 문서를 재생하지 못하는 대신, **하나의 문서 or 문장 전체를 단 하나의 벡터로 표현**할 수 있음.\n",
    "- 단어 모음 벡터는 피아노 건반 전체를 두드리는 것이 아닌, **듣기 좋은 화음을 연주**하는 셈.\n",
    "    - **서로 잘 어울리는 음(의미상 연관이 있는 단어)들의 집합을 표현**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 특정 단어의 존재 여부를 알려주는 이진 벡터를 구축해보자.\n",
    "    - 이런 이진 벡터는 어떤 단어가 어떤 문서에 쓰였는지 말해주는 문서 검색 indexing에 유용.\n",
    "    - 특정 단어가 어디에 있는지 알려주는 것이 아닌, 어떤 문장 or 문서에서 나왔는지를 말해주는 것.\n",
    "- 앞서 생성한 sentence를 이진 단어 모음 벡터로 압축해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T00:31:52.599649Z",
     "start_time": "2021-08-11T00:31:52.581661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('23.', 1),\n",
       " ('Charlie', 1),\n",
       " ('Data', 1),\n",
       " ('Science', 1),\n",
       " ('age', 1),\n",
       " ('at', 1),\n",
       " ('of', 1),\n",
       " ('started', 1),\n",
       " ('study', 1),\n",
       " ('the', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bow = {}\n",
    "\n",
    "# sentence를 공백기준으로 나눠서\n",
    "for token in sentence.split():\n",
    "    # 해당 단어가 한번 출현했으면 Dict에 추가.\n",
    "    # 이진 단어모음 벡터 (이진 희소 벡터)가 Dict에 저장되어 메모리 낭비가 그리 심하지 않음.\n",
    "    sentence_bow[token] = 1\n",
    "\n",
    "sorted(sentence_bow.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이진 벡터가 Dict 자료형에 저장되어서, 희소행렬 보다 저장공간이 훨씬 절약됨.\n",
    "    - 값이 1인 항목만을 저장.\n",
    "    - 따라서 어휘가 수천개~수만개 라도 문서에 출현한 단어에 대해서만 저장공간이 소비됨.\n",
    "- 이 Dict의 **각 항목에 해당 단어가 어휘의 몇 번째 단어인지 말해주는 index값을 넣어준다**면 저장공간을 더 줄일 수 있음.\n",
    "- 이를 위한 자료형이 바로 **pd.Series**.\n",
    "    - Series를 DataFrame으로 감싸면, 이진 벡터 말뭉치(corpus)에 더 많은 문장 추가가 가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T00:36:59.733329Z",
     "start_time": "2021-08-11T00:36:59.720322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Charlie</th>\n",
       "      <th>started</th>\n",
       "      <th>Data</th>\n",
       "      <th>Science</th>\n",
       "      <th>study</th>\n",
       "      <th>at</th>\n",
       "      <th>the</th>\n",
       "      <th>age</th>\n",
       "      <th>of</th>\n",
       "      <th>23.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Charlie  started  Data  Science  study  at  the  age  of  23.\n",
       "sent        1        1     1        1      1   1    1    1   1    1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. List comprehension으로 items를 생성한 후\n",
    "# 2. 이를 dict 자료형으로 변환,\n",
    "# 3. 이를 다시 pd.Series로 변환한 후,\n",
    "# 4. 임의의 sent를 column으로 하는 pd.DataFrame을 생성. (.T를 통해 Transpose해서 보기 좋게)\n",
    "df = pd.DataFrame(pd.Series(dict([(token, 1) for token in sentence.split()])), columns=['sent']).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T00:37:25.900881Z",
     "start_time": "2021-08-11T00:37:25.886877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Charlie', 1),\n",
       " ('started', 1),\n",
       " ('Data', 1),\n",
       " ('Science', 1),\n",
       " ('study', 1),\n",
       " ('at', 1),\n",
       " ('the', 1),\n",
       " ('age', 1),\n",
       " ('of', 1),\n",
       " ('23.', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, 1) for token in sentence.split()] # list comprehension으로 items 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 몇개를 더 추가해 앞서 정의한 df가 어떻게 성장하는지 살펴보자.  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T00:44:55.233919Z",
     "start_time": "2021-08-11T00:44:55.228935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Charlie started Data Science study at the age of 23.\\nStudy was done because of graduation project.\\nHe graduated Korea Aerospace University at 2021.\\nAnd he started studying Data Science again and again.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"\"\"Charlie started Data Science study at the age of 23.\\n\"\"\"\n",
    "sentences += \"\"\"Study was done because of graduation project.\\n\"\"\"\n",
    "sentences += \"\"\"He graduated Korea Aerospace University at 2021.\\n\"\"\"\n",
    "sentences += \"\"\"And he started studying Data Science again and again.\"\"\"\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T01:13:15.959345Z",
     "start_time": "2021-08-11T01:13:15.952511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence0': {'Charlie': 1,\n",
       "  'started': 1,\n",
       "  'Data': 1,\n",
       "  'Science': 1,\n",
       "  'study': 1,\n",
       "  'at': 1,\n",
       "  'the': 1,\n",
       "  'age': 1,\n",
       "  'of': 1,\n",
       "  '23.': 1},\n",
       " 'sentence1': {'Study': 1,\n",
       "  'was': 1,\n",
       "  'done': 1,\n",
       "  'because': 1,\n",
       "  'of': 1,\n",
       "  'graduation': 1,\n",
       "  'project.': 1},\n",
       " 'sentence2': {'He': 1,\n",
       "  'graduated': 1,\n",
       "  'Korea': 1,\n",
       "  'Aerospace': 1,\n",
       "  'University': 1,\n",
       "  'at': 1,\n",
       "  '2021.': 1},\n",
       " 'sentence3': {'And': 1,\n",
       "  'he': 1,\n",
       "  'started': 1,\n",
       "  'studying': 1,\n",
       "  'Data': 1,\n",
       "  'Science': 1,\n",
       "  'again': 1,\n",
       "  'and': 1,\n",
       "  'again.': 1}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = {}\n",
    "\n",
    "# 개행문자 \\n를 기준으로\n",
    "# 각 문장별로 토큰 분리, 분리한 토큰을 corpus[sentence (0~3)] 에 넣는다. 말뭉치에 넣기.\n",
    "for i, sent in enumerate(sentences.split('\\n')):\n",
    "    corpus[f'sentence{i}'] = dict((tok, 1) for tok in sent.split())\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T01:13:19.171512Z",
     "start_time": "2021-08-11T01:13:19.154914Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Charlie</th>\n",
       "      <th>started</th>\n",
       "      <th>Data</th>\n",
       "      <th>Science</th>\n",
       "      <th>study</th>\n",
       "      <th>at</th>\n",
       "      <th>the</th>\n",
       "      <th>age</th>\n",
       "      <th>of</th>\n",
       "      <th>23.</th>\n",
       "      <th>...</th>\n",
       "      <th>Korea</th>\n",
       "      <th>Aerospace</th>\n",
       "      <th>University</th>\n",
       "      <th>2021.</th>\n",
       "      <th>And</th>\n",
       "      <th>he</th>\n",
       "      <th>studying</th>\n",
       "      <th>again</th>\n",
       "      <th>and</th>\n",
       "      <th>again.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sentence0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Charlie  started  Data  Science  study  at  the  age  of  23.  ...  \\\n",
       "sentence0        1        1     1        1      1   1    1    1   1    1  ...   \n",
       "sentence1        0        0     0        0      0   0    0    0   1    0  ...   \n",
       "sentence2        0        0     0        0      0   1    0    0   0    0  ...   \n",
       "sentence3        0        1     1        1      0   0    0    0   0    0  ...   \n",
       "\n",
       "           Korea  Aerospace  University  2021.  And  he  studying  again  and  \\\n",
       "sentence0      0          0           0      0    0   0         0      0    0   \n",
       "sentence1      0          0           0      0    0   0         0      0    0   \n",
       "sentence2      1          1           1      1    0   0         0      0    0   \n",
       "sentence3      0          0           0      0    1   1         1      1    1   \n",
       "\n",
       "           again.  \n",
       "sentence0       0  \n",
       "sentence1       0  \n",
       "sentence2       0  \n",
       "sentence3       1  \n",
       "\n",
       "[4 rows x 28 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 앞서 생성한 말뭉치(corpus)를 DataFrame으로 변경\n",
    "# fillna를 통해 다른 문장에 없는 토큰은 0으로 대체.\n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Product\n",
    "\n",
    "<br>\n",
    "\n",
    "- inner product는 NLP에서 아주 자주 쓰이므로 개념을 확실이 이해하자.\n",
    "- **내적(inner product)** : 두 연산 대상(벡터 또는 2차원 행렬)의 '안쪽' 차원이 같아야 한다는 제약 때문에 생긴 연산.\n",
    "    - **벡터의 경우 두 벡터의 성분 개수가 같아야** 함.\n",
    "    - **행렬의 경우 왼쪽 행렬의 col 수와 오른쪽 행렬의 row 수가 같아야** 함.\n",
    "        - **INNER JOIN 연산**으로 생각하면 됨.\n",
    "    - **결과는 하나의 스칼라 값.** 그래서 **스칼라 곱(scala product)**이라고도 함.\n",
    "    - 연산이 dot ($\\cdot$) 으로 이뤄지기 때문에 **dot product** 라고도 함.\n",
    "    - 연산? **같은 위치에 있는 두 성분을 각각 곱하고, 그 결과들을 모두 더해서 나온 스칼라 값이 결과값.**\n",
    "        - 행렬 내적은 matrix product. **np.matmul** 이나 **@ 연산자**로 계산할 수 있음.\n",
    "- 내적과 대비되는 연산으로 **outer product (cross product)**가 있음. 연산자가 $\\times$ 라서.\n",
    "    - 결과는 벡터.\n",
    "- 예시를 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T01:06:09.881701Z",
     "start_time": "2021-08-11T01:06:09.873890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3] [2 3 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = np.array([1,2,3])\n",
    "v2 = np.array([2,3,4])\n",
    "print(v1, v2)\n",
    "\n",
    "v1.dot(v2) # 단순 내적.\n",
    "\n",
    "(v1 * v2).sum() # 마찬가지로 단순 내적. 같은 결과.\n",
    "\n",
    "sum([x1 * x2 for x1, x2 in zip(v1, v2)]) # 같은 결과긴 하지만 매우 느림. 쓰지말자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 두 단어 모음의 중복 측정\n",
    "\n",
    "<br>\n",
    "\n",
    "- **두 단어 모음 벡터가 얼마나 겹치는지 측정**한다면, 해당 **문장들이 단어들을 얼마나 비슷하게 사용하는지 알 수 있음.**\n",
    "- 이렇게 측정한 측도는 **두 문장의 의미가 얼마나 비슷한지를 어느정도 말**해주게 되는 셈.\n",
    "- 내적을 통해 원래 예제 문장(sentence0 ~ sentence3)과 몇가지 새 문장의 단어 모음 벡터 중복도를 측정해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T01:14:52.330355Z",
     "start_time": "2021-08-11T01:14:52.316686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence0</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Charlie</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>started</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Data</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Science</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>study</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23.</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Study</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>was</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>done</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>because</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graduation</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>project.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>He</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graduated</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Korea</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aerospace</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>University</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>And</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>studying</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>again</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>again.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sentence0  sentence1  sentence2  sentence3\n",
       "Charlie             1          0          0          0\n",
       "started             1          0          0          1\n",
       "Data                1          0          0          1\n",
       "Science             1          0          0          1\n",
       "study               1          0          0          0\n",
       "at                  1          0          1          0\n",
       "the                 1          0          0          0\n",
       "age                 1          0          0          0\n",
       "of                  1          1          0          0\n",
       "23.                 1          0          0          0\n",
       "Study               0          1          0          0\n",
       "was                 0          1          0          0\n",
       "done                0          1          0          0\n",
       "because             0          1          0          0\n",
       "graduation          0          1          0          0\n",
       "project.            0          1          0          0\n",
       "He                  0          0          1          0\n",
       "graduated           0          0          1          0\n",
       "Korea               0          0          1          0\n",
       "Aerospace           0          0          1          0\n",
       "University          0          0          1          0\n",
       "2021.               0          0          1          0\n",
       "And                 0          0          0          1\n",
       "he                  0          0          0          1\n",
       "studying            0          0          0          1\n",
       "again               0          0          0          1\n",
       "and                 0          0          0          1\n",
       "again.              0          0          0          1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.T # 다시 원래형태로 Transpose\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T01:14:57.777040Z",
     "start_time": "2021-08-11T01:14:57.772158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(df['sentence0'].dot(df['sentence1']))\n",
    "print(df['sentence0'].dot(df['sentence2']))\n",
    "print(df.sentence0.dot(df.sentence3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    결과는 두 문장 sentence0-sentence1~3이 각각 1,1,3개의 단어를 공통으로 사용함을 말해줌."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이런 단어 중복도는 두 문장의 **유사성**에 관한 측정도.\n",
    "- 해당 내적 값이 1 이상이 되게 한 단어는 뭔지 한번 찾아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T01:18:43.548491Z",
     "start_time": "2021-08-11T01:18:43.539704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('of', 1)]\n",
      "[('at', 1)]\n",
      "[('started', 1), ('Data', 1), ('Science', 1)]\n"
     ]
    }
   ],
   "source": [
    "print([(key, value) for (key, value) in (df['sentence0'] & df.sentence1).items() if value])\n",
    "print([(key, value) for (key, value) in (df['sentence0'] & df.sentence2).items() if value])\n",
    "print([(key, value) for (key, value) in (df['sentence0'] & df.sentence3).items() if value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    이것이 바로 벡터 공간 모형. Vector Space Model. VSM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이런 VSM의 단어 모음 벡터들에 대해 내적 연산 뿐만 아니라 사칙연산, 논리연산을 적용할 수 있고, 거리 or 각도 계산도 할 수 있음.\n",
    "- 문서를 이처럼 이진 벡터로 표현하는 것은 아주 강력함. 이게 문서 조회 및 검색의 주된 수단이었음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰 개선\n",
    "\n",
    "<br>\n",
    "\n",
    "- 어떤 경우엔 단어들을 공백이 아닌 **다른 문자로 구분**하는 것이 나을 수도 있음.\n",
    "    - 앞서 생성한 토큰 생성기는 26. 을 하나의 토큰으로 간주했음.\n",
    "    - 이런 문제를 해결하기 위해선 **공백 문자 뿐만이 아닌 마침표, 쉼표, 따옴표 같은 부호들도 구분자로 써야함.**\n",
    "    - 독립 토큰으로 다뤄야 할 때도 있고, 아예 제거해야 할 때도 있으니 적절하게.\n",
    "- 앞서 나온 26. 토큰은 마침표가 붙여져서 바람직하지 않은 토큰이 되었음.\n",
    "- 이런 형태는 **NLP 파이프라인 이후 단계들을 혼란에 빠뜨릴 위험이 크다.**\n",
    "- 후행 마침표를 제거하고 토큰을 뽑는 방법을 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T01:36:58.256973Z",
     "start_time": "2021-08-11T01:36:58.251114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Charlie',\n",
       " 'started',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'study',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '23',\n",
       " '']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sentence = 'Charlie started Data Science study at the age of 23.'\n",
    "\n",
    "# 정규표현식 이용.\n",
    "# 주어진 문장을 공백(\\s)이나 몇가지 문장부호(.,;!?)를 기준으로 분할.\n",
    "tokens = re.split(r'[-\\s.,;!?]+', sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    이용한 정규표현식 : [-\\s.,;!?]+\n",
    "    \n",
    "- 대괄호 **[ ]** : 주어진 텍스트가 부합해야 할 문자들의 집합 정의. character class (문자 부류) 라 부름.\n",
    "- 대괄호 옆 **+** : 주어진 문자 부류의 문자들이 **하나 이상 부합해야 함(한번이라도 나와야 한다)**을 의미.\n",
    "- **\\s** : 다양한 공백문자들을 대표. **[\\s] == [  \\t\\n\\r\\f\\v]** 와 같은 의미.\n",
    "- 대괄호 안 단일 **-** : 범위 지정이 아닌 **단일 문자 지정**.\n",
    "    - **왼쪽 대괄호 바로 옆에 지정**하거나, **앞에 \\ 를 붙여서** 써야 함.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 대괄호 안에 **문자 범위**를 지정할 수도 있다.\n",
    "    - **[0-9]** : 0부터 9까지의 십진 숫자.\n",
    "    - **[_a-zA-Z]** : 밑줄문자, 모든 영어 대소문자 부합.\n",
    "- **소괄호** : 정규 표현식의 **한 부분을 하나의 그룹으로 지정**하는 용도로 사용.\n",
    "    - 소괄호 그룹은 **전체가 문자열의 일부와 부합**해야 함.\n",
    "        - 소괄호 쌍 안의 표현식 전체가 현재 위치의 문자들과 부합해야 소괄호 쌍 다음의 패턴으로 넘어감.\n",
    "        \n",
    "<br>\n",
    "    \n",
    "- **re.split** : **두번째 인수로 문자열**을 받아, **해당 문자열을 훑으면서 정규 표현식에 지정된 패턴과 부합하는 부분 문자열을 찾음.**\n",
    "    - 부합하는 부분을 발견하면, 부합한 마지막 문자 이전 부분을 결과 집합에 추가, 이 과정을 반복.\n",
    "    - str.split() 처럼 동작하지만, 단순한 구분자가 아닌 정규 표현식을 구분 기준으로 적용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 분리를 위한 정규 표현식 개선 - re.compile\n",
    "\n",
    "<br>\n",
    "\n",
    "- 정규표현식을 한번 컴파일하면 여러곳에서 재사용이 가능.\n",
    "- re.compile() 을 통해 표현식을 미리 컴파일, 결과를 하나의 인수로서 사용하면 된다.\n",
    "- 코드 실행속도가 눈에띄게 빨라지는 건 아니지만, 표현식이 많아지는 경우 속도의 이득이 생길 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T01:52:36.957680Z",
     "start_time": "2021-08-11T01:52:36.951830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Charlie',\n",
       " ' ',\n",
       " 'started',\n",
       " ' ',\n",
       " 'Data',\n",
       " ' ',\n",
       " 'Science',\n",
       " ' ',\n",
       " 'study',\n",
       " ' ',\n",
       " 'at',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'age',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " '23',\n",
       " '.',\n",
       " '']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(r'([-\\s.,;!?])+')\n",
    "tokens = pattern.split(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    마침표 분리는 성공적인데, 불필요한 공백과 문자부호가 토큰이 되어버림.\n",
    "    한번 걸러보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T01:54:04.130792Z",
     "start_time": "2021-08-11T01:54:04.124935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Charlie',\n",
       " 'started',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'study',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '23']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in tokens if x and x not in '- \\t\\n.,;!?'] # 공백문자 (whitespace) 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T01:54:51.176823Z",
     "start_time": "2021-08-11T01:54:51.170985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Charlie',\n",
       " 'started',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'study',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '23']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x if x and x not in '- \\t\\n.,;!?' else None, tokens)) # 얘도 동일한 결과."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    re 도 좋지만, 이보다 더 좋은 regex를 쓰자. re랑 완전호환 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이렇게 쓰는 토큰 생성기는 예외사항이 생길수록 금방 복잡해짐.\n",
    "- 마침표를 구분자로 쓰면 문장들은 잘 구분하지만, 숫자의 소수점을 이상하게 분리해버리는 결과가 나올 수 있음.\n",
    "- 또는, 이모티콘에 마침표가 들어가는 경우도 있음. ^.^ 이런거. 찢어버리면 안되용...\n",
    "\n",
    "<br>\n",
    "\n",
    "- 이를 위해 토큰화 기능을 구현한 라이브러리가 여러개 있음. 대표적인 것만 살펴보면 다음과 같다.\n",
    "    - spaCy : 정확함, 유연함, 빠름, Python\n",
    "    - CoreNLP : 더 정확하지만 덜 유연함. 빠름, Java 8에 의존.\n",
    "    - NLTK : 여러곳에서 쓰이는 표준 라이브러리, 유명함, Python\n",
    "- NLTK를 써서 앞서 썻던 문장을 토큰화 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:02:03.101377Z",
     "start_time": "2021-08-11T02:02:03.095532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Charlie',\n",
       " 'started',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'study',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '23',\n",
       " '.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 만들은 정규 표현식보다 좀 더 유연하게 잘 토큰화 했다.\n",
    "- NLTK 패키지엔 TreebankWordTokenizer라는 함수도 있음. RegexpTokenizer보다 강력함.\n",
    "    - Penn Treebank tokenization에 기초한 함수. 영어 단어 토큰화에 쓰이는 다양한 규칙을 담고있음.\n",
    "        - 문장 끝 부호를 인접 토큰들과 분리하면서도 소수점 있는 수치는 하나의 토큰으로 유지.\n",
    "        - 축약형 단어들을 위한 규칙도 갖추고 있음. wasn't $\\rightarrow$ was, n't 로 토큰화. \n",
    "- 예시를 한번 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:12:14.559702Z",
     "start_time": "2021-08-11T02:12:14.554802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Monticello',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'designated',\n",
       " 'as',\n",
       " 'UNESCO',\n",
       " 'World',\n",
       " 'Heritage',\n",
       " 'Site',\n",
       " 'until',\n",
       " '1987',\n",
       " '.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "sentence = \"Monticello wasn't designated as UNESCO World Heritage Site until 1987.\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 축약형\n",
    "\n",
    "<br>\n",
    "\n",
    "- wasn't 를 was와 n't로 분리하는 것이 바람직함. 왜그럴까?\n",
    "    - 구문 트리 파서가 입력을 미리 정해진 규칙들에 기초해 일관된 토큰들로 다루기 위해선 was와 not으로 분리해야 함.\n",
    "    - 축약형 단어를 구 구성 단어들로 분리하면 모든 가능한 축약형 단어를 예측할 필요 없이 개별 단어의 철자 변형들만 고민하면 됨.\n",
    "    - 그래서 의존성 트리 파서나 구문 파서를 만들기 쉬워짐. 이런 이유 때문. 음..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # 이런거도 있답니다. casual_tokenize\n",
    "    # SNS에서 흔히 볼수 있는 비형식적이고 이모지가 난무하는 텍스트를 다루기 위한 NLTK의 패키지.\n",
    "    # 텍스트에서 사용자 이름을 제거하고, 토큰 안에서 반복되는 문자들을 줄여줌."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram을 이용한 어휘 확장\n",
    "\n",
    "<br>\n",
    "\n",
    "- 다음과 같은 문장을 생각해보자.\n",
    "\n",
    "        I scream, you scream, we all scream for ice cream.\n",
    "\n",
    "- 너와 나 우리 모두가 바라는 것은 ice cream. ice나 cream이 아님. 하나의 단위로 저장되게 하는 방법이 필요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 해결책은 바로 N-gram을 사용하는 것.\n",
    "\n",
    "<br>\n",
    "\n",
    "- *n-gram*은 **요소들의 순차열(보통 문자열)에서 추출한 최대 n개의 요소로 이뤄진 순차열.**\n",
    "    - ice cream은 2-gram이 되는 셈.\n",
    "    - Johann Sebastian Bach 는 3-gram이 되는 셈.\n",
    "- n-gram 단어들이 **반드시 어떤 특별한 의미를 가지는 것은 아님.**\n",
    "    - 복합어가 아닌 단어 조합도 얼마든지 n-gram이 될 수 있음. \n",
    "    - n-gram은 단순히 **일련의 요소 중 인접한 몇개의 요소를 묶은 것.**\n",
    "    \n",
    "<br>\n",
    "\n",
    "- n-gram이 왜 필요할까?\n",
    "    - **토큰들을 하나의 단어모음 벡터로 표현하게 되면 단어 순서에 담긴 정보가 사라지게 됨.**\n",
    "    - 하나로 된 토큰이라는 개념을 여러 단어로 이뤄진 토큰인 n-gram으로 확장하면 파이프라인은 **문장 단어 순서에 담겨진 의미를 좀 더 많이 유지**할 수 있음.\n",
    "        - 부정을 뜻하는 not은 다른 단어와 붙어있을 때 그 의미가 뚜렷해 짐.\n",
    "        - n-gram tokenization을 하지 않으면 not은 그냥 홀로 떨어진 단어.\n",
    "        - 2-gram인 'was not'은 각자 개별로 있을때보다 더 많은 의미를 내포하고 있음.\n",
    "        - 즉, 파이프라인에서 **인접 단어들을 묶으면 각 단어의 문맥(context)이 어느정도 형성됨.**\n",
    "\n",
    "<br>\n",
    "\n",
    "- 예시를 한번 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:44:04.300869Z",
     "start_time": "2021-08-11T02:44:04.295023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Charlie',\n",
       " 'started',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'study',\n",
       " 'at',\n",
       " 'the',\n",
       " 'age',\n",
       " 'of',\n",
       " '23']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존의 1-그램 토큰 생성기.\n",
    "sentence = 'Charlie started Data Science study at the age of 23.'\n",
    "pattern = re.compile(r'([-\\s.,;!?])+')\n",
    "tokens = [x for x in tokens if x and x not in '- \\t\\n.,;!?']\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:44:53.481989Z",
     "start_time": "2021-08-11T02:44:53.476112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('Charlie', 'started'),\n",
       "  ('started', 'Data'),\n",
       "  ('Data', 'Science'),\n",
       "  ('Science', 'study'),\n",
       "  ('study', 'at'),\n",
       "  ('at', 'the'),\n",
       "  ('the', 'age'),\n",
       "  ('age', 'of'),\n",
       "  ('of', '23')],\n",
       " [('Charlie', 'started', 'Data'),\n",
       "  ('started', 'Data', 'Science'),\n",
       "  ('Data', 'Science', 'study'),\n",
       "  ('Science', 'study', 'at'),\n",
       "  ('study', 'at', 'the'),\n",
       "  ('at', 'the', 'age'),\n",
       "  ('the', 'age', 'of'),\n",
       "  ('age', 'of', '23')])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk의 n-그램 토큰화 함수 사용\n",
    "from nltk.util import ngrams\n",
    "list(ngrams(tokens, 2)), list(ngrams(tokens, 3)) # 그냥 단순히 토큰과 n을 인자로 넣어주면 된다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # 참고\n",
    "            ngrams 함수는 파이썬의 generator 객체를 return함.\n",
    "            generator는 iterator처럼 작동하는 함수. \n",
    "            모든 n-gram의 순차열을 한번에 돌려주지 않고,\n",
    "            한번에 하나씩만 yield(산출) 함.\n",
    "            \n",
    "            이러면 뭐가 좋을까? for loop에서 유용함.\n",
    "            generator를 통해 순차열의 모든 요소를 메모리에 담지 않고, \n",
    "            개별 하나씩만 가져와 처리가 가능.\n",
    "            \n",
    "            return된 n-그램들을 한번에 조사하고 싶다면 산출 결과를 list로 만들면 됨.\n",
    "            (But 큰 문서에 대한 실제 토큰화 작업에는 시간이 오래걸림. 바람직하지 않다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 산출한 각 n-그램은 하나의 tuple.\n",
    "- 한 n-그램의 모든 토큰을 연결해 하나의 문자열로 만드는 것 또한 가능.\n",
    "- 이렇게 만들면 파이프라인 이후 단계들이 일관되게 문자열을 입력으로 받게 되므로 설계가 단순해짐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T02:49:26.345376Z",
     "start_time": "2021-08-11T02:49:26.339519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Charlie started',\n",
       " 'started Data',\n",
       " 'Data Science',\n",
       " 'Science study',\n",
       " 'study at',\n",
       " 'at the',\n",
       " 'the age',\n",
       " 'age of',\n",
       " 'of 23']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_grams = list(ngrams(tokens, 2))\n",
    "[' '.join(x) for x in two_grams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문제점이 있다.\n",
    "    - Data Sciene 같은 2-gram은 많은 곳에서 등장하겠지만,\n",
    "    - of 23, started Data 같은 것이 출현하는 문서는 극히 드물 것.\n",
    "- 어떤 토큰이나 n-gram이 **극히 드물게만 나타난다**는 의미는, **해당 토큰이 다른 단어들과 상관관계가 거의 없다**는 의미.\n",
    "- 즉, **희소한 n-gram은 분류 문제에 큰 도움이 되지 않는다.**\n",
    "    - 대부분의 2-gram은 상당히 드물고, 3-gram과 4-gram은 더 드물다.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 단어 조합이 개별 단어보다 훨씬 sparse하므로, 어휘 크기는 말뭉치의 모든 문서에 있는 n-gram들의 수에 지수적으로 접근한다.\n",
    "    - 특징 벡터의 차원 수가 문서의 길이보다 크면 특징 추출 단계가 오히려 파이프라인 생산성에 해를 미칠 수 있음\n",
    "    - 그런 경우 ML 모형이 벡터들에 overfitting하는 결과를 피하기가 거의 불가능.\n",
    "        - chapter 3.에서 문서 빈도 통계량 등을 이용해 ML에서 유용하지 않은 sparse n-gram을 골라내는 방법을 논의함.\n",
    "- at the 같은 경우는 드문 조합이 아니지만, **너무 많은 문서에 등장함**. **문서 의미 구분에 큰 도움이 X**.\n",
    "- **너무 자주나오는 토큰이나 n-gram은 무시**하는 것이 좋음. **불용어 필터링.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop-word (불용어)\n",
    "\n",
    "<br>\n",
    "\n",
    "- **아주 자주 출현하지만, 문구 의미에 관한 실질적 의미는 별로 담고 있지 않는 단어.**\n",
    "    - a, an, the, this 이런거.\n",
    "- 불용어 자체는 정보를 별로 전달안하지만, **n-그램에 포함되면 중요환 관계 정보를 제공할 수 있음.**\n",
    "    - Mark reported to the CEO\n",
    "    - 4-그램 단위로 보면 reported to the CEO가 만들어짐.\n",
    "    - 불용어 to, the를 지우면 reported CEO가 되서 관계 구조 정보가 사라짐.\n",
    "    - 하지만 **불용어를 사용하려면 n-그램의 n이 길어지는 단점**이 있음.\n",
    "- 불용어 필터의 구체적 **설계는 프로그램 마다 다름**. 불용어는 전체 어휘의 극히 일부분만 차지. \n",
    "\n",
    "<br>\n",
    "\n",
    "- 토큰화 과정에서 불용어들을 무조건 제거하기로 했다면, 파이썬의 list comprehension을 써서 간단히 제거 가능.\n",
    "- 예시를 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T05:20:05.997453Z",
     "start_time": "2021-08-11T05:20:05.983785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['house', 'fire']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = ['a', 'an', 'the', 'on', 'of', 'off', 'this', 'is']\n",
    "tokens = ['the', 'house', 'is', 'on', 'fire']\n",
    "tokens_without_stopwords = [x for x in tokens if x not in stop_words]\n",
    "tokens_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문장에 따라 불용어들이 담은 의미 양이 다를 수 있음. 자명.\n",
    "- 표준적인 불용어 목록을 보고자 한다면 NLTK에 정의된 목록을 한번 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T05:21:29.338082Z",
     "start_time": "2021-08-11T05:21:29.036396Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\skdbs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(179,\n",
       " ['i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  \"you're\"])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "len(stop_words), stop_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NLP 관점에서, 1인칭 문장들이 계속 나오는 문서는 상당히 지루하고, 정보량이 적은 경우가 많음.\n",
    "- sklearn의 불용어 집합과 NLTK의 불용어 집합을 함께 섞어서 사용할 수도 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어휘 정규화 (Normalization)\n",
    "\n",
    "<br>\n",
    "\n",
    "- 어휘의 크기를 줄이는 또 다른 기법.\n",
    "- **비슷한 토큰들을 하나의 정규화된 형태로 결합**하는 것.\n",
    "- 어휘의 **토큰 개수가 줄어들고**, **같은 의미지만 철자가 다른 토큰 or n-gram들을 동일한 의미 단위로 취급** 가능.\n",
    "- **Overfitting 가능성 또한 작아짐**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Folding (대소문자 합치기)\n",
    "\n",
    "<br>\n",
    "\n",
    "- **대소문자 구성만 다른 단어들을 하나로 통합**하는 것.\n",
    "    - 왜 필요? **하나의 단어가 위치 or 저자의 의도에 따라 대소문자 구성이 달라지기 때문**.\n",
    "        - 첫글자를 대문자로 쓰거나, 강조를 위해 모두 대문자로 쓰는 경우들.\n",
    "- 불규칙한 대소문자 구성들을 하나의 구성으로 정규화 한다는 점에서, **case normalization (대소문자 정규화)**라고도 함.\n",
    "- 대소문자 구성을 정규화하는 것은 **어휘의 크기를 줄이고 NLP 파이프라인을 일반화** 하는 방법 중 하나.\n",
    "- 또한, **같은 의미를 가진 여러 단어를 하나의 토큰으로 병합**하는데 도움이 됨.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 하지만 대소문자 구성이 나름의 의미를 지닐때도 있음. \n",
    "    - **고유 명사** 같은 경우. \n",
    "    - **개체명을 인식하는 것이 파이프라인의 주 과제라면, 대소문자 구성은 유지**하는 것이 좋음.\n",
    "    - 하지만 유지하면 어휘가 약 2배 커지며, 그만큼 메모리와 처리 시간도 2배로 커짐.\n",
    "\n",
    "<br>\n",
    "\n",
    "- list comprehension을 통해 대소문자 구성을 쉽게 정규화 할 수 있음.\n",
    "    - 문서 **전체에서 정규화 하는것이 바람직함이 확실**한 경우, **원문 텍스트 전체에 대해 lower()를 한 후 토큰화를 진행**하면 된다.\n",
    "- 하지만 이러면 **Camel Case 표기법으로 된 단어들을 쉽게 토큰화 하기가 힘들어짐.**\n",
    "    - Age를 검색하면 age를 검색할때와는 다른 문서들이 나옴.\n",
    "    - Age는 New Age 같은 문구에 등장하지만 age는 at the age of 같은 문구에서 등장.\n",
    "    - 어휘를 정규화 하게 되면, 사용자가 입력한 검색어의 대소문자 구성과는 무관하게 age에 관한 문서만 반환.\n",
    "- 좀 더 고급지게 한다면 첫단어만 소문자로 만들고, 나머지는 대소문자 구성을 유지하는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T05:53:08.841558Z",
     "start_time": "2021-08-11T05:53:08.835700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['house', 'visitor', 'center']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = ['House', 'Visitor', 'Center']\n",
    "norm_tokens = [x.lower() for x in tokens]\n",
    "norm_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    정규화는 재현율을 높여주긴 하지만, 정밀도를 낮춤.\n",
    "    사용자가 관심 없는 문서들이 검색 결과에 많이 포함됨.\n",
    "    --> 이런걸 피하고자 구글 검색에 \" \" 로 감싸서 감색하는 기능같은게 있음.\n",
    "    대소문자 구성을 정규화한 n그램들과 원래 대소문자 구성을 유지한 n그램으로 색인 유지."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming (어간 추출)\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "    https://wikidocs.net/21707\n",
    "- **단어 끝의 복수형 접미사나 소유격 접미사에 의한 차이를 제거**하는 것.\n",
    "- **한 단어의 여러 변형에 공통으로 존재하는 어간을 식별해 단어들을 정규화 하는 작업**.\n",
    "    - housing과 houses의 공통 어간은 house.\n",
    "    - 어간이 반드시 영어 사전에 나오는 정확한 사전일 필요는 X.\n",
    "    - NLP에서의 어간은 **한 단어의 여러 철자 변형들을 대표하는 하나의 토큰 또는 이름표** 일 뿐.\n",
    "- 장점? **문서에 담긴 정보와 의미를 최대한 유지하면서 어휘의 크기를 줄임**. 일종의 **차원축소 기법**. **모형 일반화**.\n",
    "- 키워드 검색, 정보 조회에서 중요. 관련 문서나 웹 페이지가 누락되지 않도록 검색 결과를 확장하는데 도움을 줌.\n",
    "    - developing houses in Portland 를 검색했을 때, *houses가 있는 문서 뿐만이 아닌 house나 housing이라는 문서도 검색 결과에 포함되는 이유는 **hous** 라는 하나의 공통 어간을 추출해서* 나오는 것.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 어간추출 때문에 검색 엔진의 **정밀도가 크게 감소**할 수 있음. 무관한 문서도 나올 수 있다는 것.\n",
    "    - **False-Positive(거짓양성)비율이 증가**할 수 있음.\n",
    "    - 이도 마찬가지로 \" \" 처럼 검색옵션을 통해 해결할 수 있음.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 후행 s들을 처리하는 간단한 어간 추출기 예시를 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T06:22:08.505527Z",
     "start_time": "2021-08-11T06:22:08.497717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('house', 'doctor house call')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem(phrase):\n",
    "    return ' '.join([re.findall('^(.*ss|.*?)(s)?$', word)[0][0].strip(\"'\")\n",
    "                    for word in phrase.lower().split()])\n",
    "stem('houses'), stem(\"Doctor House's calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^(.*ss|.*?)(s)?&#36; 뭘까 이게?\n",
    "    \n",
    "- 만일 단어가 둘 이상의 s로 끝나면, 어간은 그 단어 자체이고 접미사는 빈 문자열.\n",
    "- 만일 단어가 1개의 s로 끝나면, 어간은 단어에서 s를 제외한 부분이고 접미사는 s.\n",
    "- 만일 단어가 s로 끝나지 않으면, 어간은 그 단어 자체이고 접미사는 X.\n",
    "\n",
    "<br>\n",
    "\n",
    "    하지만 이 함수는 복잡한 경우엔 잘 작동을 못함. housing 같은 경우.\n",
    "    nltk의 PorterStemmer(Porter Algorithm)를 쓰면 쉽게 추출할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T06:26:13.389005Z",
     "start_time": "2021-08-11T06:26:13.382171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dish washer wash dish'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "' '.join([stemmer.stem(w).strip(\"'\") for w in \"dish washer's washed dishes\".split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization (표제어 추출)\n",
    "\n",
    "<br>\n",
    "\n",
    "- 단어들을 **그 바탕 의미를 담은 어근(root) 수준으로 내려가 정규화** 하는 작업.\n",
    "- **바탕 의미가 같지만 철자가 다른 여러 단어에 대해 파이프라인이 일관된 반응을 하는데 크게 도움**을 줌.\n",
    "- 이 또한 **대응할 단어 수를 줄임**. 언어 모형의 **차원 수를 줄이는 차원축소 기법**. **모형 일반화 가능**.\n",
    "    - 대신 **모형이 좀 덜 정확**해짐. *뿌리가 같지만 의기마 완전히 같지는 않은 철자 변형을 모두 같은 단어로 봐서*.\n",
    "        - chat, chatter, chatty, chatting, chatbot 을 똑같이 취급해버림.\n",
    "    - 의도적으로 이렇게 만들 수도 있음. 이를 spoofing 이라고 함.\n",
    "\n",
    "<br>\n",
    "\n",
    "- 의미를 고려해 단어를 정규화 하므로 **어간 추출이나 대소문자 정규화보다 좀 더 정확한 정규화 방법**.\n",
    "- 동의어들과 단어 어미들에 관한 **지식 베이스를 활용**해 **실제 의미가 비슷한 단어들만 하나의 토큰으로 통합**.\n",
    "- 단어의 철자 뿐만 아닌 **품사(part of speech, POS)도 활용**해 **정밀도를 개선**하기도 함.\n",
    "    - 이럴땐 **단어가 어떤 역할을 하는지 품사 태그(꼬리표)를 단어에 부여**함. \n",
    "        - 예전 예제들 돌려봤을때 단어마다 'v', 'n' 이렇게 붙어있던거.\n",
    "\n",
    "<br>\n",
    "\n",
    "- ***품사에 기초한 표제어 추출이 어간 추출보다 더 나은 어근을 식별할 수 있는 이유가 뭘까.***\n",
    "    - better를 생각해보자.\n",
    "    - 어간 추출기는 어미 er를 제거해 bett나 bet 같은 어간을 추출할 것.\n",
    "    - 이렇게 하면 better는 bets나 Bet's 처럼 전혀 다른 의미의 단어들과 묶임.\n",
    "    - 하지만 표제어 추출기는 better를 betterment, best 처럼 의미상으로 가까운 단어들과 묶음.\n",
    "- 그래서 대부분의 어플리케이션에선 표제어 추출기보다 어간 추출기보다 나음.\n",
    "    - 어간 추출기는 대규모 정보 검색 어플리케이션 같은 곳에서나 쓰임.\n",
    "- **어간 추출 바로 앞 단계에 표제어 추출 단계를 배치**해 더 나은 결과를 얻을 수도 있음.\n",
    "    - 즉, 어간 추출기 앞에 표제어 추출기를 두게 되면, 어간 추출기만 사용했을 때 보다 **차원이 더 많이 축소되고 정보 검색 재현율도 높아지게 된다.**\n",
    "\n",
    "<br>\n",
    "\n",
    "- 표제어 식별 예제를 보자. NLTK의 WordNetLemmatizer라는 추출기가 있음.\n",
    "    - **두번째 인수**로 **단어의 품사 정보**를 요구함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-11T06:49:07.491783Z",
     "start_time": "2021-08-11T06:49:07.483972Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\skdbs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('good', 'goods', 'goodness', 'goodness')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('better', pos='a'), lemmatizer.lemmatize('goods', pos='a'), lemmatizer.lemmatize('goodness', pos='a'), lemmatizer.lemmatize('goodness', pos='n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    품사를 명시적으로 지정하지 않으면 WordNetLemmatizer는 그 단어가 명사라고 가정함.\n",
    "    이 추출기는 프린스턴 대학교의 WordNet 단어 의미 그래프에 있는 단어 연결 관계만을 이용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적으로 어간 추출이 표제어 추출보다 빠르고, 필요한 코드와 자료집합도 덜 복잡.\n",
    "- 하지만 어간 추출기는 표제어 추출기보다 실수가 더 잦고, 더 많은 단어를 하나의 토큰으로 합침. 정보손실 증가."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
