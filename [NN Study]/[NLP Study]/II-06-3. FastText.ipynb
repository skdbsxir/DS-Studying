{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940a6565",
   "metadata": {},
   "source": [
    "# FastText\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Word2Vec의 확장**으로 볼 수 있음.\n",
    "- 차이점? Word2Vec은 단어를 쪼개질 수 없는 단위로 생각하는 반면, **FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주.**\n",
    "    - **내부 단어 (subword)를 고려**해 학습."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b0967",
   "metadata": {},
   "source": [
    "## 내부 단어(subword)의 학습\n",
    "\n",
    "<br>\n",
    "\n",
    "- FastText에선 **각 단어를 글자 단위 n-gram의 구성으로 취급.**\n",
    "    - n을 몇으로 결정하는지에 따라 단어들이 얼마나 분리되는지 결정됨.\n",
    "    - ***시작과 끝을 의미하는 <, > 를 도입.*** 예시를 보자.\n",
    "\n",
    "```{.python}\n",
    "# apple에 대해 n = 3인 경우, 단어 분리 후 벡터 생성\n",
    "<ap, app, ppl, ple, le>\n",
    "```\n",
    "- 추가적으로 **기존 단어에 <, >를 붙인 토큰을 하나 더 벡터화** 함. **특별 토큰**.\n",
    "```{.python}\n",
    "# 특별토큰\n",
    "<apple>\n",
    "```\n",
    "- 즉, n=3인 경우 FastText는 *단어 apple에 대해 5개의 일반 토큰과 1개의 특별 토큰을 벡터화* 한다.\n",
    "```{.python}\n",
    "<ap, app, ppl, ple, le>, <apple>\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "- 실 사용시엔 n의 최소값과 최대값으로 범위를 설정할 수 있음. default는 3,6.\n",
    "    - 최소값 = 3, 최대값 = 6인 경우, apple에 대해 FastText는 다음과 같은 단어들을 벡터화 한다.\n",
    "    ```{.python}\n",
    "    # n = 3 ~ 6 인 경우\n",
    "    <ap, app, ppl, ppl, le>, <app, appl, pple, ple>, <appl, pple>, ... , <apple>\n",
    "    ```\n",
    "    - **내부 단어를 벡터화 한다는 의미는 이 단어들에 대해 Word2Vec을 수행한다**는 의미.\n",
    "    - 내부 단어들의 벡터값을 얻었다면, apple의 벡터값은 위의 **벡터값들의 총 합**으로 구성.\n",
    "    ```{.python}\n",
    "    apple = <ap + app + ppl + ppl + le> + <app + appl + , ..., +<apple> \n",
    "    ```\n",
    "- 이런 방법은 Word2Vec에선 가질 수 없었던 강점을 가지게 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41fa0cd",
   "metadata": {},
   "source": [
    "## 모르는 단어(Out Of Vocabulary, OOV)에 대한 대응\n",
    "\n",
    "<br>\n",
    "\n",
    "- FastText의 신경망을 학습한 후엔, 데이터 셋의 모든 단어의 각 n-gram에 대해 워드 임베딩이 진행됨.\n",
    "- Word2Vec(and GloVe)에 비해 생기는 강점이 뭘까?\n",
    "    - 데이터 셋만 충분하다면, **내부단어를 통해 모르는 단어에 대해서도 다른 단어와의 유사도 계산이 가능.**\n",
    "    - birthplace라는 단어를 학습하지 않은 상태라고 해보자.\n",
    "    - 다른 단어에서 ***birth와 place라는 내부 단어***가 있다면,\n",
    "    - FastText는 **birthplace의 벡터를 얻을 수 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b9637",
   "metadata": {},
   "source": [
    "## 단어 집합 내 빈도수가 적었던 단어(Rare Word)에 대한 대응\n",
    "\n",
    "<br>\n",
    "\n",
    "- Word2Vec은 등장 빈도수가 적은 단어에 대해서 임베딩 정확도가 높지 않았었음.\n",
    "    - **참고할 수 있는 경우의 수가 적다보니 정확하게 임베딩이 되지 않았던 것.**\n",
    "- FastText는 **단어가 희귀 단어라도, 그 단어의 n-gram이 다른 단어의 n-gram과 겹치는 경우**라면\n",
    "- Word2Vec과 비교했을 때 **비교적 높은 임베딩 벡터 값을 얻을 수 있음.**\n",
    "\n",
    "<br>\n",
    "\n",
    "- 이런 점 때문에 FastText는 **노이즈가 많은 말뭉치에서 강점**을 갖게 됨.\n",
    "- 실제 많은 비정형 데이터엔 오타나 맞춤법이 틀린 단어가 많이 섞여있음.\n",
    "    - ***이런 단어들은 당연히 등장 빈도수가 매우 적음.*** 일종의 희귀 단어.\n",
    "    - Word2Vec에서 이런 단어는 임베딩이 제대로 안됐지만, FastText는 일정 수준의 성능을 보여줌."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cef5fe",
   "metadata": {},
   "source": [
    "## Word2Vec vs. FastText 간단 비교\n",
    "\n",
    "<br>\n",
    "\n",
    "- 이전에 썼던 ted xml파일 그대로 사용. 다시 코드 쓰자 ㅠ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69af0eb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T01:38:58.260407Z",
     "start_time": "2021-08-18T01:38:56.542000Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Cheol Hee\n",
      "[nltk_data]     Kim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from lxml import etree\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2bcf75b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T01:40:56.848832Z",
     "start_time": "2021-08-18T01:39:58.119154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수 : 273424\n"
     ]
    }
   ],
   "source": [
    "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "targetText = etree.parse(targetXML)\n",
    "\n",
    "# context block 내용만 추출,\n",
    "# regex써서 괄호 배경음 부분 제거.\n",
    "parseText = '\\n'.join(targetText.xpath('//content/text()'))\n",
    "contentText = re.sub(r'\\([^)]*\\)', '', parseText)\n",
    "\n",
    "# 입력 corpus에 대해 NLTK를 통해 문장 토큰화 진행 (불용어 제거)\n",
    "sentText = sent_tokenize(contentText)\n",
    "\n",
    "# 구두점 제거, 대문자->소문자 변환.\n",
    "normalizedText = []\n",
    "for text in sentText:\n",
    "    tokens = re.sub(r\"^a-z0-9\", \" \", text.lower())\n",
    "    normalizedText.append(tokens)\n",
    "    \n",
    "# 각 문장 토큰화 진행\n",
    "result = [word_tokenize(sentence) for sentence in normalizedText]\n",
    "\n",
    "print('총 샘플 수 : {}'.format(len(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8e0781",
   "metadata": {},
   "source": [
    "### Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b9e517b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T01:42:55.840686Z",
     "start_time": "2021-08-18T01:42:38.499043Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences = result,\n",
    "    vector_size = 100,\n",
    "    window = 5,\n",
    "    min_count = 5,\n",
    "    workers = 4,\n",
    "    sg = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e869f9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T01:43:49.970323Z",
     "start_time": "2021-08-18T01:43:49.455597Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'electrofishing' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CHEOLH~1\\AppData\\Local\\Temp/ipykernel_11992/1450797880.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'electrofishing'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\WorkBase\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    760\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                     \u001b[0mall_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\WorkBase\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \"\"\"\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\WorkBase\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    394\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'electrofishing' not present\""
     ]
    }
   ],
   "source": [
    "model.wv.most_similar('electrofishing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b460b58",
   "metadata": {},
   "source": [
    "    단어 집합에 electrofishing이 없다고 함.\n",
    "    Word2Vec은 학습 데이터에 존재하지 않는 단어(모르는 단어)에 대해선\n",
    "    임베딩 벡터가 존재하지 않으므로 유사도 계산이 안됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff99cceb",
   "metadata": {},
   "source": [
    "### FastText\n",
    "\n",
    "<br>\n",
    "\n",
    "- Word2Vec 학습 코드만 FastText로 변경해 실행해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41b12d9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T01:47:25.686918Z",
     "start_time": "2021-08-18T01:46:02.253501Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText(\n",
    "    result,\n",
    "    vector_size = 100,\n",
    "    window = 5,\n",
    "    min_count = 5,\n",
    "    workers = 4,\n",
    "    sg = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca8b0de0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-18T01:48:22.423188Z",
     "start_time": "2021-08-18T01:48:22.391306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('electrolux', 0.8584972023963928),\n",
       " ('electrolyte', 0.8558030724525452),\n",
       " ('electroshock', 0.8482690453529358),\n",
       " ('electroencephalogram', 0.8378998637199402),\n",
       " ('airbag', 0.8358634114265442),\n",
       " ('electric', 0.8316196799278259),\n",
       " ('airbus', 0.8284210562705994),\n",
       " ('electrogram', 0.8203827738761902),\n",
       " ('electron', 0.8109163641929626),\n",
       " ('electrode', 0.8094940185546875)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('electrofishing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304dfe7d",
   "metadata": {},
   "source": [
    "    Word2Vec과는 다르게\n",
    "    학습하지 않은 단어에 대해 유사한 단어를 계산해 출력하고 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad7739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'[WorkBase]'",
   "language": "python",
   "name": "workbase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
